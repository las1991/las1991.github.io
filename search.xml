<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL慢查询日志]]></title>
    <url>%2F2019%2F10%2F23%2FMySQL%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[查看是否开启慢查询功能12345678mysql&gt; show variables like &apos;slow_query%&apos;;+---------------------+------------------------------------+| Variable_name | Value |+---------------------+------------------------------------+| slow_query_log | OFF || slow_query_log_file | /var/lib/mysql/instance-1-slow.log |+---------------------+------------------------------------+2 rows in set (0.01 sec) 1234567mysql&gt; show variables like &apos;long_query_time&apos;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+1 row in set (0.00 sec) 说明： slow_query_log 慢查询开启状态 slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） long_query_time 查询超过多少秒才记录 配置临时配置默认没有开启慢查询日志记录，通过命令临时开启： 12345678mysql&gt; set global slow_query_log=&apos;ON&apos;;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global slow_query_log_file=&apos;/var/lib/mysql/instance-1-slow.log&apos;;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global long_query_time=2;Query OK, 0 rows affected (0.00 sec) 永久配置修改配置文件达到永久配置状态：12345/etc/mysql/conf.d/mysql.cnf[mysqld]slow_query_log = ONslow_query_log_file = /var/lib/mysql/instance-1-slow.loglong_query_time = 2 配置好后，重新启动 MySQL 即可。测试通过运行下面的命令，达到问题 SQL 语句的执行： 1234567mysql&gt; select sleep(2);+----------+| sleep(2) |+----------+| 0 |+----------+1 row in set (2.00 sec) 然后查看慢查询日志内容： 123456789101112$ cat /var/lib/mysql/instance-1-slow.log/usr/sbin/mysqld, Version: 8.0.13 (MySQL Community Server - GPL). started with:Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sockTime Id Command Argument/usr/sbin/mysqld, Version: 8.0.13 (MySQL Community Server - GPL). started with:Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sockTime Id Command Argument# Time: 2018-12-18T05:55:15.941477Z# User@Host: root[root] @ localhost [] Id: 53# Query_time: 2.000479 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0SET timestamp=1545112515;select sleep(2);]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL查看数据库表容量大小]]></title>
    <url>%2F2019%2F10%2F23%2FMySQL%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%AE%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[1. 查看所有数据库容量大小12345678select table_schema as &apos;数据库&apos;,sum(table_rows) as &apos;记录数&apos;,sum(truncate(data_length/1024/1024, 2)) as &apos;数据容量(MB)&apos;,sum(truncate(index_length/1024/1024, 2)) as &apos;索引容量(MB)&apos;from information_schema.tablesgroup by table_schemaorder by sum(data_length) desc, sum(index_length) desc; 2. 查看所有数据库各表容量大小12345678select table_schema as &apos;数据库&apos;,table_name as &apos;表名&apos;,table_rows as &apos;记录数&apos;,truncate(data_length/1024/1024, 2) as &apos;数据容量(MB)&apos;,truncate(index_length/1024/1024, 2) as &apos;索引容量(MB)&apos;from information_schema.tablesorder by data_length desc, index_length desc; 3.查看指定数据库容量大小1234567select table_schema as &apos;数据库&apos;,sum(table_rows) as &apos;记录数&apos;,sum(truncate(data_length/1024/1024, 2)) as &apos;数据容量(MB)&apos;,sum(truncate(index_length/1024/1024, 2)) as &apos;索引容量(MB)&apos;from information_schema.tableswhere table_schema=&apos;mysql&apos;; 4.查看指定数据库各表容量大小123456789select table_schema as &apos;数据库&apos;,table_name as &apos;表名&apos;,table_rows as &apos;记录数&apos;,truncate(data_length/1024/1024, 2) as &apos;数据容量(MB)&apos;,truncate(index_length/1024/1024, 2) as &apos;索引容量(MB)&apos;from information_schema.tableswhere table_schema=&apos;mysql&apos;order by data_length desc, index_length desc;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的可靠性传输？]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BC%A0%E8%BE%93%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。 如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。 数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。 RabbitMQ 生产者弄丢了数据生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。123456789101112// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 Kafka消费端弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。]]></content>
      <tags>
        <tag>MQ</tag>
        <tag>Kafka</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的顺序性？]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何保证消息的顺序性？我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。]]></content>
      <tags>
        <tag>MQ</tag>
        <tag>Kafka</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码字符的发展史]]></title>
    <url>%2F2019%2F09%2F03%2F%E7%BC%96%E7%A0%81%E5%AD%97%E7%AC%A6%E7%9A%84%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[原文链接 鼻祖，ascii，7位(bit)范围128 计算机发明后，为了在计算机中表示字符，人们制定了一种编码，叫ASCII码。ASCII码由一个字节中的7位(bit)表示，范围是0x00 - 0x7F 共128个字符。 随之出现扩展ascii,8位范围256 后来他们突然发现，如果需要按照表格方式打印这些字符的时候，缺少了“制表符”。于是又扩展了ASCII的定义，使用一个字节的全部8位(bit)来表示字符了，这就叫扩展ASCII码。范围是0x00 - 0xFF 共256个字符。 中国最早出现gb-2312，支持中文 中国人利用连续2个扩展ASCII码的扩展区域（0xA0以后）来表示一个汉字，该方法的标准叫GB-2312。后来，日文、韩文、阿拉伯文、台湾繁体（BIG-5）……都使用类似的方法扩展了本地字符集的定义，现在统一称为 MBCS 字符集（多字节字符集）。这个方法是有缺陷的，因为各个国家地区定义的字符集有交集，因此使用GB-2312的软件，就不能在BIG-5的环境下运行（显示乱码），反之亦然。 随之出现gbk,扩展的gb-2312,包含复杂中文和繁体 使用gb-2312一段时间后，我们很快就就发现有许多人的人名没有办法在这里打出来，中国汉字真乃博大精深。于是我们不得不继续把GB2312 没有用到的码位找出来老实不客气地用上。后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。扩展之后的编码方案被称为 GBK 标准，GBK包括了GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 进一步扩展GB18030，全中国用 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS“（Double Byte Charecter Set 双字节字符集）。 统一全球，出现unicode标准字符集 统一规定2字节=1字符 之后全世界各个国家都有自己的编码标准，导致国家与国家之间的编码转换很有问题。为了把全世界人民所有的所有的文字符号都统一进行编码，于是一个叫 ISO （国际标谁化组织）的国际组织制定了”Universal Multiple-ctet Coded Character Set”，简称 UCS, 俗称 “unicode“。UNICODE 使用2个字节表示一个字符，对于ASCII里的那些英文“半角”字符，其原编码不变，只是将其长度由原来的8位扩展为16位，高位补0.而其他文化和语言的字符则全部重新统一编码。这种大气的方案在保存英文文本时会多浪费一倍的空间。这下终于好啦，全世界任何一个地区的软件，可以不用修改地就能在另一个地区运行了。虽然我用 IE 浏览日本网站，显示出我不认识的日文文字，但至少不会是乱码了。UNICODE 的范围是 0x0000 - 0xFFFF 共6万多个字符，其中光汉字就占用了4万多个 UTF-8的出现 由于字节浪费和unicode与ascii的区别，unicode在很长一段时间内无法推广，直到互联网的出现，为解决unicode如何在网络上传输的问题，于是面向传输的众多UTF（UCS Transfer Format）标准出现了，顾名思义，UTF-8就是每次8个位传输数据，而UTF-16就是每次16个位。UTF-8就是在互联网上使用最广的一种unicode的实现方式，这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。它是一种变长的编码方式，它可以使用1~4个字节表示一个符号。注意的是unicode一个中文字符占2个字节，而UTF-8一个中文字符占3个字节。 常用字符集 字符集 位数 范围 个数 作用 ASCII 7 00-7F 128 表语英语及西欧语言 ASCII扩展 8 00-FF 256 表语英语及西欧语言 ISO-8859-1 8 00-FF 256 扩展ASCII，表示西欧、希腊语等 GB2312 16 高0xA1–0xF7低0xA1–0xFE 7445个符号，包括6763个汉字 国家简体中文字符集，兼容ASCII BIG5 16 高0x81-0xFE低0x40-0x7E、0xA1-0xFE 13053个汉字 统一繁体字编码 GBK 16 高0x81-0xFE低0x40-0xFE 21886个字符 它是GB2312的扩展，加入对繁体字的支持，兼容GB2312 GB18030 8/16/32 解决了中文、日文、朝鲜语等的编码，兼容GBK UCS 16/32 国际标准 ISO 10646 定义了通用字符集 (Universal Character Set)。它是与UNICODE同类的组织，UCS-2和UNICODE兼容 Unicode 分别是UTF-8，UTF-16和UTF-32 为世界650种语言进行统一编码，兼容ISO-8859-1]]></content>
      <tags>
        <tag>编码字符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中判断字符串真实长度（中文2个字符，英文1个字符）的方法]]></title>
    <url>%2F2019%2F09%2F03%2Fjava%E4%B8%AD%E5%88%A4%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9C%9F%E5%AE%9E%E9%95%BF%E5%BA%A6%EF%BC%88%E4%B8%AD%E6%96%872%E4%B8%AA%E5%AD%97%E7%AC%A6%EF%BC%8C%E8%8B%B1%E6%96%871%E4%B8%AA%E5%AD%97%E7%AC%A6%EF%BC%89%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java中判断字段真实长度（中文2个字符，英文1个字符）的方法1234567891011121314151617public class StringLength &#123; String s1 = &quot;a&quot;; String s2 = &quot;国1a&quot;; String s3 = &quot;aa&quot;; @Test public void testLength() &#123; assert 1 == s1.getBytes().length; assert 3 == s2.length(); assert 4 == s2.getBytes(Charset.forName(&quot;GBK&quot;)).length; assert 5 == s2.getBytes().length; assert 2 == s3.getBytes().length; assert 2 == s3.getBytes(Charset.forName(&quot;GBK&quot;)).length; assert 2 == s3.length(); &#125;&#125; 为什么用GBK字符集？其实不止是gbk编码，GB2312、GB18030也是可以的，主要利用了，字符集用2个字节表示中文，1个字节表示英文、数字。当然这不是万能的，不在字符集中的字符会不准确，例如:”嗀、兀、鎴、戝、枩、綘”,GB2312用1个字节表示 英文字母和中文汉字在不同字符集编码下的字节数12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.UnsupportedEncodingException;public class EncodingTest &#123; @Test public void test() &#123; String en = &quot;A&quot;; String ch = &quot;人&quot;; System.out.println(&quot;英文字母：&quot; + en); printByteLength(en, &quot;GB2312&quot;); printByteLength(en, &quot;GBK&quot;); printByteLength(en, &quot;GB18030&quot;); printByteLength(en, &quot;ISO-8859-1&quot;); printByteLength(en, &quot;UTF-8&quot;); printByteLength(en, &quot;UTF-16&quot;); printByteLength(en, &quot;UTF-16BE&quot;); printByteLength(en, &quot;UTF-16LE&quot;); System.out.println(); System.out.println(&quot;中文汉字：&quot; + ch); printByteLength(ch, &quot;GB2312&quot;); printByteLength(ch, &quot;GBK&quot;); printByteLength(ch, &quot;GB18030&quot;); printByteLength(ch, &quot;ISO-8859-1&quot;); printByteLength(ch, &quot;UTF-8&quot;); printByteLength(ch, &quot;UTF-16&quot;); printByteLength(ch, &quot;UTF-16BE&quot;); printByteLength(ch, &quot;UTF-16LE&quot;); &#125; /** * 打印不同字符集下Java字符串所占的字节数 * * @param str 待操作的字符串 * @param encodingName 字符集名称 */ public void printByteLength(String str, String encodingName) &#123; System.out.print(&quot;字节数 : &quot;); try &#123; System.out.print(str.getBytes(encodingName).length); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;;编码：&quot; + encodingName); &#125;&#125; 结果：12345678910111213141516171819英文字母：A字节数 : 1;编码：GB2312字节数 : 1;编码：GBK字节数 : 1;编码：GB18030字节数 : 1;编码：ISO-8859-1字节数 : 1;编码：UTF-8字节数 : 4;编码：UTF-16字节数 : 2;编码：UTF-16BE字节数 : 2;编码：UTF-16LE中文汉字：人字节数 : 2;编码：GB2312字节数 : 2;编码：GBK字节数 : 2;编码：GB18030字节数 : 1;编码：ISO-8859-1字节数 : 3;编码：UTF-8字节数 : 4;编码：UTF-16字节数 : 2;编码：UTF-16BE字节数 : 2;编码：UTF-16LE]]></content>
      <tags>
        <tag>java</tag>
        <tag>String</tag>
        <tag>编码字符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP Finite State Machine]]></title>
    <url>%2F2019%2F08%2F25%2FTCP-Finite-State-Machine%2F</url>
    <content type="text"><![CDATA[TCP Operational Overview and the TCP Finite State Machine (FSM) TCP 的那些事儿(上) TCP Finite State Machine (FSM) States, Events and Transitions Table 151: TCP Finite State Machine (FSM) States, Events and Transitions State State Description Event and Transition CLOSED This is the default state that each connection starts in before the process of establishing it begins. The state is called “fictional” in the standard. The reason is that this state represents the situation where there is no connection between devices—it either hasn't been created yet, or has just been destroyed. If that makes sense. J Passive Open: A server begins the process of connection setup by doing a passive open on a TCP port. At the same time, it sets up the data structure (transmission control block or TCB) needed to manage the connection. It then transitions to the LISTEN state. Active Open, Send SYN: A client begins connection setup by sending a SYN message, and also sets up a TCB for this connection. It then transitions to the SYN-SENT state. LISTEN A device (normally a server) is waiting to receive a synchronize (SYN) message from a client. It has not yet sent its own SYN message. Receive Client SYN, Send SYN+ACK: The server device receives a SYN from a client. It sends back a message that contains its own SYN and also acknowledges the one it received. The server moves to the SYN-RECEIVED state. SYN-SENT The device (normally a client) has sent a synchronize (SYN) message and is waiting for a matching SYN from the other device (usually a server). Receive SYN, Send ACK: If the device that has sent its SYN message receives a SYN from the other device but not an ACK for its own SYN, it acknowledges the SYN it receives and then transitions to SYN-RECEIVED to wait for the acknowledgment to its SYN. Receive SYN+ACK, Send ACK: If the device that sent the SYN receives both an acknowledgment to its SYN and also a SYN from the other device, it acknowledges the SYN received and then moves straight to the ESTABLISHED state. SYN-RECEIVED The device has both received a SYN (connection request) from its partner and sent its own SYN. It is now waiting for an ACK to its SYN to finish connection setup. Receive ACK: When the device receives the ACK to the SYN it sent, it transitions to the ESTABLISHED state. ESTABLISHED The “steady state” of an open TCP connection. Data can be exchanged freely once both devices in the connection enter this state. This will continue until the connection is closed for one reason or another. Close, Send FIN: A device can close the connection by sending a message with the FIN (finish) bit sent and transition to the FIN-WAIT-1 state. Receive FIN: A device may receive a FIN message from its connection partner asking that the connection be closed. It will acknowledge this message and transition to the CLOSE-WAIT state. CLOSE-WAIT The device has received a close request (FIN) from the other device. It must now wait for the application on the local device to acknowledge this request and generate a matching request. Close, Send FIN: The application using TCP, having been informed the other process wants to shut down, sends a close request to the TCP layer on the machine upon which it is running. TCP then sends a FIN to the remote device that already asked to terminate the connection. This device now transitions to LAST-ACK. LAST-ACK A device that has already received a close request and acknowledged it, has sent its own FIN and is waiting for an ACK to this request. Receive ACK for FIN: The device receives an acknowledgment for its close request. We have now sent our FIN and had it acknowledged, and received the other device's FIN and acknowledged it, so we go straight to the CLOSED state. FIN-WAIT-1 A device in this state is waiting for an ACK for a FIN it has sent, or is waiting for a connection termination request from the other device. Receive ACK for FIN: The device receives an acknowledgment for its close request. It transitions to the FIN-WAIT-2 state. Receive FIN, Send ACK: The device does not receive an ACK for its own FIN, but receives a FIN from the other device. It acknowledges it, and moves to the CLOSING state. FIN-WAIT-2 A device in this state has received an ACK for its request to terminate the connection and is now waiting for a matching FIN from the other device. Receive FIN, Send ACK: The device receives a FIN from the other device. It acknowledges it and moves to the TIME-WAIT state. CLOSING The device has received a FIN from the other device and sent an ACK for it, but not yet received an ACK for its own FIN message. Receive ACK for FIN: The device receives an acknowledgment for its close request. It transitions to the TIME-WAIT state. TIME-WAIT The device has now received a FIN from the other device and acknowledged it, and sent its own FIN and received an ACK for it. We are done, except for waiting to ensure the ACK is received and prevent potential overlap with new connections. (See the topic describing connection termination for more details on this state.) Timer Expiration: After a designated wait period, device transitions to the CLOSED state. tcp 三次握手 四次挥手 对于建链接的3次握手，主要是要初始化Sequence Number 的初始值。通信的双方要互相通知对方自己的初始化的Sequence Number（缩写为ISN：Inital Sequence Number）——所以叫SYN，全称Synchronize Sequence Numbers。也就上图中的 x 和 y。这个号要作为以后的数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输的问题而乱序（TCP会用这个序号来拼接数据）。 对于4次挥手，其实你仔细看是2次，因为TCP是全双工的，所以，发送方和接收方都需要Fin和Ack。只不过，有一方是被动的，所以看上去就成了所谓的4次挥手。如果两边同时断连接，那就会就进入到CLOSING状态，然后到达TIME_WAIT状态。下图是双方同时断连接的示意图（你同样可以对照着TCP状态机看）:]]></content>
      <tags>
        <tag>tcp</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Registry Mirror]]></title>
    <url>%2F2019%2F08%2F24%2FDocker-Registry-Mirror%2F</url>
    <content type="text"><![CDATA[docker有官方的中国区 https://registry.docker-cn.com 网易163 docker镜像 http://hub-mirror.c.163.com/ ustc的镜像Docker Hub 源使用帮助 https://docker.mirrors.ustc.edu.cn daocloudDaoCloud也提供了docker加速器，需要用户注册后才能使用，并且每月限制流量10GB。 https://www.daocloud.io/mirror#accelerator-doc http://xxxx.m.daocloud.io aliyun阿里云也提供了docker加速器，注册为阿里云的用户，还得加入开发者平台 https://yourcode.mirror.aliyuncs.com azure http://dockerhub.azk8s.cn tencent https://mirror.ccs.tencentyun.com]]></content>
      <tags>
        <tag>docker</tag>
        <tag>mirror</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次频繁Full GC问题排查]]></title>
    <url>%2F2019%2F07%2F21%2F%E4%B8%80%E6%AC%A1%E9%A2%91%E7%B9%81Full-GC%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[频繁Full GC问题排查定位先上结论：长时间运行MetricsClientHttpRequestInterceptor.servoMonitorCache.timerCache积累过多且不回收，导致频繁fullGc 起因：运维的同事说，你这个服务运行时间长了，内存不回收啊，内存一直往上加，我们半月就得重启一次，要不就返回503了，我心想不可能啊，java自带内存回收，后来一看系统监控果然，内存大趋势随着时间线性增长，很稳定，放大之后发现，内存是有回收的，只是每次的最低点都在提高，我第一反应是，对象到老年代了，一直在养老，钉子户越来越多导致的，那就铲除钉子户吧。 jstat -gcutil [pid] 1000 查看gc执行频率，确实一直在Full GC ，而且还收不干净。 jmap -F -dump:format=b,file=heapDump [pid] 先保留现场，不能让服务一直不可用，留好快照，让运维的同事先重启解决下。 用eclipse mat 工具打开dump文件，文件太大，3.13g,需要修改mat启动参数,如图: 主要是这两行-Xmx4096m,-Xms1024m 点击Leak Suspects，查找内存泄漏 发现内存泄漏的class, org.springframework.cloud.netflix.metrics.MetricsClientHttpRequestInterceptor org.springframework.cloud.netflix.metrics.servo.ServoMonitorCache(servoMonitorCache) java.util.HashMap(timerCache) 回到源码，查找对应class,发现 Intercepts RestTemplate requests and records metrics about execution time and results. 原来是为了统计执行时间和返回结果的，那就关掉吧。 在MetricsInterceptorConfiguration中 1@ConditionalOnProperty(value = &quot;spring.cloud.netflix.metrics.enabled&quot;, havingValue = &quot;true&quot;, matchIfMissing = true) 默认是true，需要在自己的配置文件中添加spring.cloud.netflix.metrics.enabled=false 发版本，上线，观察内存监控，很平稳。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>FullGC</tag>
        <tag>jmap</tag>
        <tag>jstat</tag>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器cpu100%怎么定位？]]></title>
    <url>%2F2019%2F07%2F21%2F%E6%9C%8D%E5%8A%A1%E5%99%A8cpu100-%E6%80%8E%E4%B9%88%E5%AE%9A%E4%BD%8D%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？ 找到最耗CPU的进程 执行top -c ，显示进程运行信息列表 键入P (大写p)，进程按照CPU使用率排序 如上图，最耗CPU的进程PID为29025 找到最耗CPU的线程 top -Hp 29025 ，显示一个进程的线程运行信息列表 键入P (大写p)，线程按照CPU使用率排序 如图 进程29025 最耗cpu的线程pid是29327，29119 将线程PID转化为16进制 printf “%x,%x” 29327 29119 输出 “728f,71bf” 之所以要转化为16进制，是因为堆栈里，线程id是用16进制表示的。 查看堆栈，找到线程在干嘛 jstack 29025 | grep -e “728f” -e “71bf” -C5 —color 如上图找到cpu高的线程对应的线程名字为 “netty-worker-2-1”，”netty-worker-2-2”，以及看到了该线程正在执行代码的堆栈。]]></content>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 单例模式中双重检查锁定 volatile 的作用]]></title>
    <url>%2F2019%2F07%2F16%2Fjava-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B8%AD%E5%8F%8C%E9%87%8D%E6%A3%80%E6%9F%A5%E9%94%81%E5%AE%9A-volatile-%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[volatile 是保证了可见性还是有序性？有序性：是因为 instance = new Singleton(); 不是原子操作。编译器存在指令重排，从而存在线程1 创建实例后（初始化未完成），线程2 判断对象不为空，但实际对象扔为空，造成错误。 可见性：是因为线程1 创建实例后还只存在自己线程的工作内存，未更新到主存。线程 2 判断对象为空，创建实例，从而存在多实例错误。 结论主要是禁止重排序，初始化一个实例（SomeType st = new SomeType()）在java字节码中会有4个步骤， 申请内存空间 初始化默认值（区别于构造器方法的初始化） 执行构造器方法 连接引用和实例 这4个步骤后两个有可能会重排序，1234 1243都有可能，造成未初始化完全的对象发布。volatile可以禁止指令重排序，从而避免这个问题。 为什么要禁止重排序？确保先执行构造器方法，再将引用和实例连接到一起。如果没有禁止重排序，会导致另一个线程可能获取到尚未构造完成的对象。 为什么没有起到可见性的作用？ JSR-133 An unlock on a monitor happens before every subsequent lock on that same monitor 第二次非null判断是在加锁以后，则根据这一条，另一个线程一定能看到这个引用被赋值。所以即使没有volatile，依旧能保证可见性。 如果不加volatile，能不能使代码正确运行？既然可见性已经有了保证，那我们只需要保证有序性。怎么保证有序性呢？ 只需要在“构造对象”和“连接引用与实例”之间加上一道内存屏障 由于是在单线程里，同样根据JSR-133 Each action in a thread happens before every action in that thread that comes later in the program’s order DoubleCheckedLocking]]></content>
      <tags>
        <tag>java</tag>
        <tag>单例模式</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F07%2F09%2FHashMap%2F</url>
    <content type="text"><![CDATA[HashMap数据结构JDK1.8以前HashMap的实现是 数组+链表JDK1.8开始HashMap的实现是 数组+链表/红黑树，如下图： HashMap类中有两个常量：12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; 当链表中节点数量大于等于TREEIFY_THRESHOLD时，链表会转成红黑树。 当链表中节点数量小于等于UNTREEIFY_THRESHOLD时，红黑树会转成链表。 为什么TREEIFY_THRESHOLD的默认值被设定为8？HashMap中有这样一段注释:12345678910111213141516171819202122232425/* Because TreeNodes are about twice the size of regular nodes, we * use them only when bins contain enough nodes to warrant use * (see TREEIFY_THRESHOLD). And when they become too small (due to * removal or resizing) they are converted back to plain bins. In * usages with well-distributed user hashCodes, tree bins are * rarely used. Ideally, under random hashCodes, the frequency of * nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average for the default resizing * threshold of 0.75, although with a large variance because of * resizing granularity. Ignoring variance, the expected * occurrences of list size k are (exp(-0.5) * pow(0.5, k) / * factorial(k)). The first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million */ 当hashCode离散性很好的时候，树型bin用到的概率非常小，因为数据均匀分布在每个bin中，几乎不会有bin中链表长度会达到阈值。但是在随机hashCode下，离散性可能会变差，然而JDK又不能阻止用户实现这种不好的hash算法，因此就可能导致不均匀的数据分布。不过理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，我们可以看到，一个bin中链表长度达到8个元素的概率为0.00000006，几乎是不可能事件。所以，之所以选择8，不是拍拍屁股决定的，而是根据概率统计决定的。]]></content>
      <tags>
        <tag>java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effects of Type Erasure and Bridge Methods]]></title>
    <url>%2F2019%2F07%2F04%2FEffects-of-Type-Erasure-and-Bridge-Methods%2F</url>
    <content type="text"><![CDATA[原文链接 Effects of Type ErasureSometimes type erasure causes a situation that you may not have anticipated. The following example shows how this can occur. The example (described in Bridge Methods) shows how a compiler sometimes creates a synthetic method, called a bridge method, as part of the type erasure process. Given the following two classes: 1234567891011121314151617181920public class Node&lt;T&gt; &#123; public T data; public Node(T data) &#123; this.data = data; &#125; public void setData(T data) &#123; System.out.println(&quot;Node.setData&quot;); this.data = data; &#125;&#125;public class MyNode extends Node&lt;Integer&gt; &#123; public MyNode(Integer data) &#123; super(data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125;&#125; Consider the following code:1234MyNode mn = new MyNode(5);Node n = mn; // A raw type - compiler throws an unchecked warningn.setData(&quot;Hello&quot;); Integer x = mn.data; // Causes a ClassCastException to be thrown. After type erasure, this code becomes:1234MyNode mn = new MyNode(5);Node n = (MyNode)mn; // A raw type - compiler throws an unchecked warningn.setData(&quot;Hello&quot;);Integer x = (String)mn.data; // Causes a ClassCastException to be thrown. Here is what happens as the code is executed: n.setData(“Hello”); causes the method setData(Object) to be executed on the object of class MyNode. (The MyNode class inherited setData(Object) from Node.) In the body of setData(Object), the data field of the object referenced by n is assigned to a String. The data field of that same object, referenced via mn, can be accessed and is expected to be an integer (since mn is a MyNode which is a Node. Trying to assign a String to an Integer causes a ClassCastException from a cast inserted at the assignment by a Java compiler. Bridge MethodsWhen compiling a class or interface that extends a parameterized class or implements a parameterized interface, the compiler may need to create a synthetic method, called a bridge method, as part of the type erasure process. You normally don’t need to worry about bridge methods, but you might be puzzled if one appears in a stack trace. After type erasure, the Node and MyNode classes become: 123456789101112131415161718192021public class Node &#123; public Object data; public Node(Object data) &#123; this.data = data; &#125; public void setData(Object data) &#123; System.out.println(&quot;Node.setData&quot;); this.data = data; &#125;&#125;public class MyNode extends Node &#123; public MyNode(Integer data) &#123; super(data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125;&#125; After type erasure, the method signatures do not match. The Node method becomes setData(Object) and the MyNode method becomes setData(Integer). Therefore, the MyNode setData method does not override the Node setData method. To solve this problem and preserve the polymorphism of generic types after type erasure, a Java compiler generates a bridge method to ensure that subtyping works as expected. For the MyNode class, the compiler generates the following bridge method for setData: 123456789101112131415class MyNode extends Node &#123; // Bridge method generated by the compiler // public void setData(Object data) &#123; setData((Integer) data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125; // ...&#125; As you can see, the bridge method, which has the same method signature as the Node class’s setData method after type erasure, delegates to the original setData method.]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm-方法调用]]></title>
    <url>%2F2019%2F07%2F04%2Fjvm-%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[重载与重写重载在java程序里，如果同一个类中出现多个名字相同，并且参数类型相同的方法，那么它无法通过编译。也就是说，在正常情况下，如果我们想要在同一个类中定义名字相同的方法，那么它们的参数类型必须不同。这些方法之间的关系，我们称之为重载。 重载的方法在编译过程中即可完成识别。具体到每一个方法调用，java编译器会根据所传入参数的声明类型（注意与实际类型区分）来选取重载方法。选取的过程共分为三个阶段： 在不考虑对基本类型自动拆装箱（auto-boxing，auto-unboxing），以及可变长参数的情况下选取重载方法； 如果在第1个阶段中没有找到适配的方法，那么在允许自动拆装箱，但不允许可变长参数的情况下选取重载方法； 如果在第2阶段中没有找到适配的方法，那么在允许自动拆装箱以及可变长的情况下选取重载方法。 如果java编译器在同一阶段中找到了多个适配的方法，那么它会在其中选择一个最为贴切的，而决定贴切程度的一个关键就是形式参数类型的继承关系。例如： void invoke(Object,Object... args){...} void invoke(String s,Object obj,Object... args){...}第一个参数当传入null时，由于String是Object的子类，因此java编译器会认为第二个方法更为贴切。 重写如果子类定义了与父类中非私有方法同名的方法，而且这个方法的参数类型相同，并且这两个方法都不是静态的，那么子类的方法重写了父类中的方法。 jvm的静态绑定和动态绑定java虚拟机识别方法的关键在于类名、方法名以及方法描述符（method descriptor）。方法描述符是由方法的参数类型以及返回类型所构成。如果同一个类中同时出现多个名字相同且描述符也相同的方法，那么java虚拟机会在类的验证阶段报错。 对于java语言中重写而java虚拟机中非重写的情况，编译器会通过类型擦除和方法桥接来实现java中的重写语义。 由于对重载方法的区分在编译阶段已经完成，所以java虚拟机不存在重载概念。 在java虚拟机中，静态绑定指的是在解析时便能够直接识别目标方法的情况，而动态绑定指的是需要在运行过程中根据调用者的动态类型来识别目标方法的情况。 java字节码中与调用相关的指令有5种： invokestatic：用于调用静态方法。 invokespecial：用于调用私有实例方法、构造器，以及使用super关键字调用父类的实例方法或构造器，和所有接口的默认方法。 invokevirtual：用于调用非私有实例方法。 invokeinterface：用于调用接口方法。 invokedynamic：用于调用动态方法。 虚方法调用对于invokevirtual和invokeinterface而言，在绝大部分年情况下，虚拟机需要在执行过程中，根据调用者的动态类型，来确定具体的目标方法。这两种指令均属于java虚拟机中的虚方法调用，这个过程也称为动态绑定。相对于静态绑定的非虚方法调用来说，虚方法调用更加耗时。唯一的例外在于，如果虚拟机能够确定目标方法有且仅有一个，比如虚方法调用目标方法被标记为final方法，那么也可以不通过动态绑定，使用静态绑定该虚调用的目标方法。 对于invokestatic和invokespecial而言，java虚拟机能够直接识别具体的目标方法。这个过程属于静态绑定 方法表java虚拟机中采取了一种空间换取时间的策略来实现动态绑定，在类加载的准备阶段，它除了为静态字段分配内存，还会构造与该类相关联的方法表，用以快速定位目标方法。 invokevirtual使用虚方法表（virtual method table，vtable），invokeinterface使用接口方法表（interface method table，itable），接口方法表稍微复杂，但原理类似。 方法表本质是一个数组每个数组元素指向一个当前类及其祖先类中非私有实例方法。 子类方法表包含父类方法表的所有方法； 子类方法表在方法表中的索引值与它所重写的父类方法的索引值相同。 实际上，使用了方法表的动态绑定与静态绑定相比，仅仅多出几个内存解引用操作。访问栈上的调用者，读取调用者的动态类型，读取该类型的方法表，读取方法表中某个索引值所对应的目标方法。相对于创建并初始化java栈帧来说，这几个内存解引用操作的开销简直可以忽略不计。 内联缓存方法表优化实际仅存在于解释执行中，或者即时编译代码的最坏情况。即时编译还拥有另外两种性能更好的优化手段：内联缓存（inlining cache）和方法内联（method inlining）。 内联缓存是一种加快动态绑定的优化技术。它能够缓存虚方法调用中调用者的动态类型，以及该类型所对应的目标方法。在之后的执行过程中，如果碰到已缓存的类型，内联缓存便会直接调用该类型所对应的目标方法。如果没有碰到已缓存的类型，内联缓存则会退化至使用基于方法表的动态绑定。 单态（monomorphic）指的是仅有一种状态的情况。 多态（polymorphic）指的是有限数量种状态的情况。二态（bimorphic）是多态的其中一种。 超多态（megamorphic）指的是更多种状态的情况。通常用一个具体数值来区分多态和超多态。 当内联缓存没有命中的情况下，java虚拟机需要重新使用方法表进行动态绑定。对于内联缓存中的内容，java虚拟机选择劣化为超多态，处于这种状态下的内联缓存，实际上放弃了优化的机会。直接访问方法表，来动态绑定目标方法。 单态内联缓存和超多态内联缓存的性能差距 Conditions for inlining methods by the HotSpot VM It doesn’t matter how many sub-class a class has, the only thing which matter is how many methods could be called from a given line of code. e.g. a method could have two implementations across four class, but if only one is called, it will be as if the methods had only one implementation. Performance of inlined virtual method invocations in Java]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm-类加载机制]]></title>
    <url>%2F2019%2F07%2F01%2Fjvm-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类加载过程 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括了：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（using）、和卸载（Unloading）七个阶段。其中验证、准备和解析三个部分统称为连接（Linking），如下如所示。 加载，是指查找字节流，并据此创建类的过程。 链接 验证，确保被加载的类能够满足java虚拟机的约束条件。 准备，为被加载类的静态字段分配内存。 解析，将符号引用解析为实际引用。如果符号引用指向一个未被加载的类或未被加载类的字段或方法，那么解析将触发这个类的加载（但未必出发这个类的链接和初始化。） 初始化，为标记为常量值的字段赋值，以及执行clinit方法。 clinit（如果直接赋值的静态字段被final所修饰，并且它的类型是基本类型或字符串时，那么该字段便会被java编译器标记成常量，其初始化直接有java虚拟机完成。除此之外的直接赋值操作，以及所有静态代码块中的代码，则会被java编译器置于同一方法中，并把它命名为clinit） 类的初始化何时会被出发，JVM规范枚举了下述情况： 当虚拟机启动时，初始化用户指定的主类。 当遇到用以新建目标实例的new指令时，初始化new指令的目标类。 当遇到调用静态方法的指令时，初始化该静态方法所在的类。 当遇到访问静态字段的指令时，初始化该静态字段所在的类。 子类初始化会触发父类的初始化。 如果接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，会触发该接口的初始化。 使用反射api对某个类进行反射调用时，初始化这个类。 当初次调用methodHandle实例时，初始化该MethodHandle指向的方法所在的类。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Happens-Before规则]]></title>
    <url>%2F2019%2F06%2F17%2FHappens-Before%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[Happens-Before的7个规则 程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而”后面”是指时间上的先后顺序。 volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的”后面”同样是指时间上的先后顺序。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join（）方法结束、Thread.isAlive（）的返回值等手段检测到线程已经终止执行。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。]]></content>
      <tags>
        <tag>java</tag>
        <tag>并发编程</tag>
        <tag>java内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程的生命周期]]></title>
    <url>%2F2019%2F06%2F14%2Fjava%E7%BA%BF%E7%A8%8B%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[通用的线程生命周期 通用的线程生命周期基本上可以用下图这个“五态模型”来描述。这五态分别是：初始状态、可运行状态、运行状态、休眠状态、终止状态 1.初始状态 线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。 2.可运行状态 线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。 3.运行状态 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了运行状态。 4.休眠状态 运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。 5.终止状态 线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。 这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 除了简化合并，这五种状态也有可能被细化，比如，Java 语言里就细化了休眠状态（这个下面我们会详细讲解）。 Java 中线程的生命周期 Java 语言中线程共有六种状态，分别是： NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） RUNNABLE 与 BLOCKED 的状态转换 只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。 如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。 而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。 RUNNABLE 与 WAITING 的状态转换 获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。其中，wait() 方法我们在上一篇讲解管程的时候已经深入介绍过了，这里就不再赘述。 调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。 调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。 RUNNABLE 与 TIMED_WAITING 的状态转换 调用带超时参数的 Thread.sleep(long millis) 方法； 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法； 调用带超时参数的 Thread.join(long millis) 方法； 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法； 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。 从 NEW 到 RUNNABLE 状态 Java 刚创建出来的 Thread 对象就是 NEW 状态，而创建 Thread 对象主要有两种方法。 继承 Thread 对象，重写 run() 方法。 123456789// 自定义线程对象class MyThread extends Thread &#123; public void run() &#123; // 线程需要执行的代码 ...... &#125;&#125;// 创建线程对象MyThread myThread = new MyThread(); 实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数。 12345678910// 实现 Runnable 接口class Runner implements Runnable &#123; @Override public void run() &#123; // 线程需要执行的代码 ...... &#125;&#125;// 创建线程对象Thread thread = new Thread(new Runner()); 从 RUNNABLE 到 TERMINATED 状态 线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。 那 stop() 和 interrupt() 方法的主要区别是什么呢？ stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了，所以这里也就不多介绍了。 而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。 上面这两种情况属于被中断的线程通过异常的方式获得了通知。还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了。 Java线程状态切换]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于长连接的负载均衡解决方案]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%85%B3%E4%BA%8E%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[最近公司培训，讲到了有关于长连接的负载均衡解决方案，确有体会，故此记录一下。在之前做负载均衡一般针对的是短连接，短连接的场景在实际应用中非常普遍。浏览器中大部分的请求都是短连接，例如用户登录，注册。商城的订单，付款等功能都属于短连接。短连接的特点就是无状态，连接时间短，长则三四秒，短则几毫秒。短连接的负载均衡很容易解决，开源中间件也比较多,例如nginx，F5等。而长连接的负载均衡解决方案则比较少了，主要原因是长连接相对于短连接来说应用面比较窄。一般是定制化需求会使用长连接。而生活中长连接作为普遍的应用就是直播系统了，近几年直播这种娱乐方式也越来越受到年轻人的喜爱，虎牙，斗鱼等直播平台如雨后春笋般涌现。我所在的公司主要从事安防行业，其中最为普遍的业务是摄像头的录像，在城市中每个街头，小区，公交地铁上的摄像头都会接入到公安体系中，其中录像不但能实时播放而且还会保存到服务器中，方便公安人员破案时可以随时查看录像。由于摄像机是24小时录像的，所以在这种场景下摄像机是使用长连接，而且一旦摄像机和某个服务器节点建立连接，就会长期和这个服务器保持着连接。一个城市的摄像头成千上万，后台不但需要考虑并发，还要考虑负载均衡。传统的负载均衡算法如轮询，哈希，随机等算法并不适用于长连接的一些业务场景。这里长连接可比作一个持续进行的任务，那么长连接的负载均衡就是每个任务的资源调度，最终使每个节点上的资源，均匀的分布在这些任务上。 长连接的特点由于业务场景不同，长连接的特点也不一样。不同的业务场景会有各种各样的特点，需要做定制化需求的开发，这里例举的摄像机长连接特点： 每个长连接任务的时间长，短则几天，长则几个月。并且长期占用服务器的资源。 每个长连接任务消耗的资源不一样。以摄像机为例，不同规格的摄像机像素不一样。有标清，高清，超清之分。对应的就是流量带宽的区别，有2M，4M，8M之分。因此每个长连接任务占用的资源大小不一样的。 长连接任务启动时，初始化较慢，例如需要做认证，或是请求一些接口来获取数据。这样会导致做负载均衡时，并不能实时获取到节点的负载情况，出现几秒钟的延时。 在这些特点下如何做负载均衡呢？短连接的负载均衡算法轮询，哈希，随机，平均负载。如果这些算法能不能直接应用于长连接的业务场景，会出现哪些问题呢？我们先从最简单的特点开始，逐渐递进的去思考这个问题，从无到有的推导出一个针对这些特点的负载均衡算法。上面三个特点以下简称特点一，特点二，特点三。 满足特点一，不考虑特点二，三这种情景下长连接任务只是时间长，但每个长连接任务消耗资源大小一样，启动时无延时。这种场景是最为简单也是最理想的场景，在这种场景下轮询，哈希，随机算法都适用。因为每个任务大小一样，当任务数量够多时就能保证任务是平均分配到节点上的，从而保证了每个节点的负载基本一致，这些算法都能使服务器节点达到较为均匀的负载状态，这种情况根短连接的情况差不多，只是任务时间长一些。如下图 满足特点一，二，不考虑特点三这种场景下任务启动时没有延时。但由于每个任务大小不一样轮询，哈希，随机就也就不再适应了。因为这几种算法适用于请求数量很大，任务大小差别不大的情况。也就是样本数足够大，样本基本一致才能保证这些样本均匀的分布在节点上。但也仅仅只是保证了每个节点上的样本数量均匀分布，倘若每个样本的所占用的资源不一样。那么这种均匀分配实际并不会使服务器的节点能够均匀的负载，达不到负载均衡的效果。那么这样的场景下，只平均负载的算法能够适用与这种场景。平均负载算法是在每次派发任务时，会收到节点反馈过来的负载情况，然后根据每个节点的负载情况，选择负载最小的节点派发任务。下面以图来说明这两种情况，如下图从图上可知，有任务a,b,c分别对应三个摄像机。任务a是2M流量，任务b是4M流量，任务c是8M流量，假设每个节点配置都一样，满载时为100。很明显如果使用轮询节点c的负载肯定会大，这样实际上集群负载并不均衡。但如果使用平均负载，那么负载均衡器在派发任务时会根据节点的负载情况去分发任务，这样最终效果肯定是要比轮询好的 满足特点一，二，三其实在前面两个特点下，我们忽略了一个重要问题。就是可信度问题，也就是特点三带来的问题。在分布式环境中，节点之间通信是不可靠。如果由于网络波动，或是业务场景导致负载均衡器在得到节点反馈过来的负载信息时，这个负载信息可能是存在几秒钟的延时。实际上得到的并不是当前的实时负载情况，而是几秒钟之前的负载情况。如下图：这样会存在一个极大的隐患。假设有1000个请求过来了。根据平均负载的算法得到节点a的负载最低。那么请求理应分发到节点a上，但是由于反馈过来的负载信息是存在延时的，于是在两三秒内得到的结果都是节点a负载最低，那么这1000个请求可能会全部被分发到节点a上。这种现象被称为扎堆效应。这种情况的结果就是节点a扛不住压力宕机。那么由此可见平均负载算法也不适用了。 平均负载算法的改进(一)从上面几种场景可以看出，平均负载算法还是适用性是非常强的，而在第三种场景下如果能得到一个可信度较高的节点当前负载情况，那么就可以继续沿用平均负载算法。那么该如何得到可信的节点负载呢，由前面的分析可知得到的节点负载之所以不可信是因为存在延时，那么我们可以记录节点前三秒，或者是前五秒的每秒节点负载，在把这些节点负载取平均值作为节点的当前负载。公式如下 m=(m4+m3+m2+m1+m0)/5m表示当前负载 ，m4,m3,m2,m1,m0分别表示4,3,2,1,0秒前的负载，对于存在延时的系统，这样做法就能在一定程度上得到较为可靠节点的负载情况。但仍然不够准确，在实际生产环境中仍然会出现不均衡的情况，究其原因是因为对前5秒的数据取平均，但实际上，这样计算是不合理的，难道5秒以前的数据和1秒以前的数据对当前结果的影响还是一样的吗？显然不是，应该认为越靠近当前时间的负载值可信度就越高，越远离当前时间的负载值可信度就越低。这样才会越接近于实际。所以上面的公式应该是这样的 m= 0.36*m0+0.28*m1+0.20*m2+0.12*m3+0.04*m4其中m0离当前时间最近，认为m0的可信度最高，所以给了36%的权重，而m4离当前时间最远，可信度最低，所以给了4%的权重。当然这里的权重应根据实际情况或经验设置值，根据这个公式算出的当前节点的负载值更能反应出节点的真实负载情况 平均负载算法的改进(二)上面两个公式是在得到几秒内的负载值然后计算出加权平均值确能在一定程度上反应节点的负载情况，但是这个种方法需要多次记录节点的负载信息，还比较麻烦。那有没有更好的方法呢，答案是有的，其实只需记录上一秒的节点负载情况，然后再获取当前节点负载，在这段时间段内对节点内，节点负载对时间取积分即可。这个方法是根据自动控制原理中的PID算法得到的那么上面节点负载的计算公式可得如下 m=m0+\frac{\int_0^t{(m0-m1)*t}\,{\rm d}t}{D}公式中m表示计算得到的负载值，m0表示当前获取到的节点负载，m1表示上一秒获取到的节点负载，参数D是可信度系数，表示上一秒的节点负载值对当前负载值的影响程度，越大表示影响越小，这个值可根据实际情况设置。这样既只需要维护上一秒的节点负载值，减小了成本，又能得到相对准确的当前节点负载值。 负载均衡健康状态负载均衡的健康状态如何定义呢？如果任务均匀的分布在各个节点上，就称集群的负载均衡状态是健康的，否则就是亚健康或者说是不健康的。在前面，我们衡量集群各个节点的负载情况时，总是以节点已经负载的量去评定的，例如集群有3个节点，每个节点的满载量为100，当前每个节点放负载为40，那么就可以认为集群达到了比较均匀的负载均衡状态。但这是理想状态，实际生活中，集群的节点配置是不可能完全相同的，有些节点可能是后加入的那配置可能会高一些，有些节点是之前用了好几年的，为了节省成本继续利用起来配置自然会点一些，所以集群各个节点是完全不一样的。如下图所示a,b,c三个节点，a节点放满载量为60，b节点的满载量为140，c节点的满载量为100，这样如继续以节点已负载的量去衡量的话，假设当前每个节点放负载仍为40，就会得到如下的情况 节点 节点a 节点b 节点c 负载状态 40/60 40/140 40/100 这样明显可以看出节点b还有很大的富余，因此通过节点当前的负载量，去判断集群的负载均衡状态是不科学的，这里可以换一种思路，以每个节点的剩余量作为衡量标准，这样上面那个例子中，节点a的富余是20，节点b的富余是100，节点c的富余是60，这样得到的就是如下的情况 节点 节点a 节点b 节点c 富余量 20 100 60 在负载均衡服务器派发任务时，不再是以节点负载最小的节点作为派发对象，而是以节点富余最大的节点作为派发对象，这样就能使节点的负载尽可能的均衡。这里衍生出一个公式可计算集群负载均衡的状态，首先计算出集群各个节点的负载富余的平均值 n=\frac{(n0+n1+n2)}3然后求方差 y=(n0-n)^2+(n1-n)^2+(n2-n)^2这样计算得到的方差结果，如果结果越小则说明集群负载越均衡，状态越健康，越大则说明越不均衡，集群的负载均衡处于一个相对不健康的状态]]></content>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg 接收rtsp时的pts dts设置]]></title>
    <url>%2F2019%2F04%2F04%2Fffmpeg-%E6%8E%A5%E6%94%B6rtsp%E6%97%B6%E7%9A%84pts-dts%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[设置ptsPts的设置是根据timestamp和rtcp的ntp time设置的，在函数finalize_packet()内设置。跟ntp time挂钩是音视频同步的基础。基本上可以认为timestamp的变化值就是pts的变化值，在没有B frame 时25fps 就是3600递增， 29.97fps 是3003 递增。存在B frame 时 timestamp 值 不是线性增加。 1234567891011121314151617181920212223242526272829303132333435363738/** * This was the second switch in rtp_parse packet. * Normalizes time, if required, sets stream_index, etc. */ static void finalize_packet(RTPDemuxContext *s, AVPacket *pkt, uint32_t timestamp)&#123; if (pkt-&gt;pts != AV_NOPTS_VALUE || pkt-&gt;dts != AV_NOPTS_VALUE) return; /* Timestamp already set by depacketizer */ if (timestamp == RTP_NOTS_VALUE) return; if (s-&gt;last_rtcp_ntp_time != AV_NOPTS_VALUE &amp;&amp; s-&gt;ic-&gt;nb_streams &gt; 1) &#123; int64_t addend; int delta_timestamp; /* compute pts from timestamp with received ntp_time */ delta_timestamp = timestamp - s-&gt;last_rtcp_timestamp; /* convert to the PTS timebase */ addend = av_rescale(s-&gt;last_rtcp_ntp_time - s-&gt;first_rtcp_ntp_time, s-&gt;st-&gt;time_base.den, (uint64_t) s-&gt;st-&gt;time_base.num &lt;&lt; 32); pkt-&gt;pts = s-&gt;range_start_offset + s-&gt;rtcp_ts_offset + addend + delta_timestamp; return; &#125; if (!s-&gt;base_timestamp) s-&gt;base_timestamp = timestamp; /* assume that the difference is INT32_MIN &lt; x &lt; INT32_MAX, * but allow the first timestamp to exceed INT32_MAX */ if (!s-&gt;timestamp) s-&gt;unwrapped_timestamp += timestamp; else s-&gt;unwrapped_timestamp += (int32_t)(timestamp - s-&gt;timestamp); s-&gt;timestamp = timestamp; pkt-&gt;pts = s-&gt;unwrapped_timestamp + s-&gt;range_start_offset - s-&gt;base_timestamp; &#125; 其中的s-&gt;st-&gt;time_base.den = 90000 / 44100 等， s-&gt;st-&gt;time_base.num = 1。 设置dtsDts的值是从pts的值里面选择的，在没有B帧时设置成和pts一样； 有B frame存在时，根据B frame num ， 缓存相应B frame num个数加1的pts， 缓存的pts按从小到大排列。然后从数组0位置取出来当dts。 12345678910111213delay = st-&gt;internal-&gt;avctx-&gt;has_b_frames;......if (pkt-&gt;pts != AV_NOPTS_VALUE &amp;&amp; delay &lt;= MAX_REORDER_DELAY) &#123; st-&gt;pts_buffer[0] = pkt-&gt;pts; for (i = 0; i&lt;delay &amp;&amp; st-&gt;pts_buffer[i] &gt; st-&gt;pts_buffer[i + 1]; i++) FFSWAP(int64_t, st-&gt;pts_buffer[i], st-&gt;pts_buffer[i + 1]); if(has_decode_delay_been_guessed(st)) pkt-&gt;dts = select_from_pts_buffer(st, st-&gt;pts_buffer, pkt-&gt;dts); &#125;]]></content>
      <tags>
        <tag>ffmpeg</tag>
        <tag>h264</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamoDB Clear Data]]></title>
    <url>%2F2019%2F03%2F14%2FDynamoDB-Clear-Data%2F</url>
    <content type="text"><![CDATA[原文 Depending on the size of your table this can be too expensive and result in downtime. Remember that deletes cost you the same as a write, so you’ll get throttled by your provisioned WCU. It would be much simpler and faster to just delete and recreate the table. 12345678# this uses jq but basically we&apos;re just removing # some of the json fields that describe an existing # ddb table and are not actually part of the table schema/defintionaws dynamodb describe-table --table-name $table_name | jq &apos;.Table | del(.TableId, .TableArn, .ItemCount, .TableSizeBytes, .CreationDateTime, .TableStatus, .ProvisionedThroughput.NumberOfDecreasesToday, .ProvisionedThroughput.LastIncreaseDateTime, .ProvisionedThroughput.LastDecreaseDateTime, .LocalSecondaryIndexes[].IndexSizeBytes, .LocalSecondaryIndexes[].IndexArn, .LocalSecondaryIndexes[].ItemCount)&apos; &gt; schema.json# delete the tableaws dynamodb delete-table --table-name $table_name# create table with same schema (including name and provisioned capacity)aws dynamodb create-table --cli-input-json file://schema.json If you really want to you can delete each item individually and you’re on the right track you just need to specify both the hash and range keys in your scan projection and delete command.1234567891011aws dynamodb scan \ --attributes-to-get $HASH_KEY $RANGE_KEY \ --table-name $TABLE_NAME --query &quot;Items[*]&quot; \ # use jq to get each item on its own line | jq --compact-output &apos;.[]&apos; \ # replace newlines with null terminated so # we can tell xargs to ignore special characters | tr &apos;\n&apos; &apos;\0&apos; \ | xargs -0 -t -I keyItem \ # use the whole item as the key to delete (dynamo keys *are* dynamo items) aws dynamodb delete-item --table-name $TABLE_NAME --key=keyItem If you want to get super fancy you can use the describe-table call to fetch the hash and range key to populate $HASH_KEY and $RANGE_KEY but i’ll leave that as an exercise for you.]]></content>
      <tags>
        <tag>aws</tag>
        <tag>dynamoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Remove MySQL completely]]></title>
    <url>%2F2019%2F03%2F09%2FRemove-MySQL-completely%2F</url>
    <content type="text"><![CDATA[Remove MySQL completely1. Open the Terminal 2. Use `mysqldump` to backup your databases 3. Check for MySQL processes with: `ps -ax | grep mysql` 4. Stop and kill any MySQL processes 5. Analyze MySQL on HomeBrew: `sss` 12brew remove mysqlbrew cleanup 6. Remove files: 123456sudo rm /usr/local/mysqlsudo rm -rf /usr/local/var/mysqlsudo rm -rf /usr/local/mysql*sudo rm ~/Library/LaunchAgents/homebrew.mxcl.mysql.plistsudo rm -rf /Library/StartupItems/MySQLCOMsudo rm -rf /Library/PreferencePanes/My* 7. Unload previous MySQL Auto-Login: 1launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist 8. Remove previous MySQL Configuration: 12subl /etc/hostconfig` # Remove the line MYSQLCOM=-YES- 9. Remove previous MySQL Preferences: 1234rm -rf ~/Library/PreferencePanes/My*sudo rm -rf /Library/Receipts/mysql*sudo rm -rf /Library/Receipts/MySQL*sudo rm -rf /private/var/db/receipts/*mysql* 10. Restart your computer just to ensure any MySQL processes are killed 11. Try to run mysql, **it shouldn&#39;t work**]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Xcode断点调试ffmpeg]]></title>
    <url>%2F2019%2F03%2F06%2F%E4%BD%BF%E7%94%A8Xcode%E6%96%AD%E7%82%B9%E8%B0%83%E8%AF%95ffmpeg%2F</url>
    <content type="text"><![CDATA[参考原文整理 ffmpeg-xcodeffmpeg xcode project build stepsstep 1 下载ffmpeg源码 FFmpeggit clone https://git.ffmpeg.org/ffmpeg.git FFmpeg step 2 编译 ffmpeg cd FFmpeg ./configure --enable-debug=trace make -j8 问题1编译时报错 yasm/nasm not found or too old. Use --disable-yasm for a crippled build. (yasm是汇编编译器, 因为ffmpeg中为了提高效率用到了汇编指令, 比如MMX和SSE) 解决方法 `brew install yasm` step 3 新建一个空的 xcode 项目 Create a new Xcode project 新建一个空的 xcode 项目 项目保存路径与FFmpeg同级. step 4 添加 FFmpeg 源码目录进 ffmpeg-debug2 项目中 1 选中FFmpeg目录 2 拖到项目目录下 3 不要勾选下面这个选项 接着一点要选 ffmpeg-debug2, 否则看代码时, 不可以跳转. 写代码时不会有提示. step 5 添加头文件搜索路径 到这里就可以实现头文件跳转了. 要等待处理完毕才可以点击头文件或者类来查看代码. $(SRCROOT)/../FFmpeg $(SRCROOT)/../FFmpeg/libavcodec $(SRCROOT)/../FFmpeg/llibavfilter $(SRCROOT)/../FFmpeg/libavdevice $(SRCROOT)/../FFmpeg/libutil $(SRCROOT)/../FFmpeg/libformat $(SRCROOT)/../FFmpeg/libswscal $(SRCROOT)/../FFmpeg/libpostproc $(SRCROOT)/../FFmpeg/libavresample $(SRCROOT)/../FFmpeg/libwresample step 6 添加一个 target File -&gt; New -&gt; Target -&gt; OS X -&gt; Other -&gt; External Build System target 命名为 ffmpeg-make step 7 修改 ffmpeg-make 源码路径配置 修改 ffmpeg-make 源码路径 step 8 修改 ffmpeg-make 命令行参数 配置命令行参数在Xcode工具栏中选择你的target然后点击Edit Scheme 配置executable 结果 step 9 大功告成 添加断点(ffplay.c-&gt;main函数), 点击 run]]></content>
      <tags>
        <tag>ffmpeg</tag>
        <tag>xcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零了解H264结构]]></title>
    <url>%2F2019%2F03%2F06%2F%E4%BB%8E%E9%9B%B6%E4%BA%86%E8%A7%A3H264%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[H264/AVC是广泛采用的一种编码方式。我们这边会带大家了解。从大到小排序依次是 序列，图像，片组，片，NALU，宏块，亚宏块，块，像素。 前言我们可以了解H264处于编解码层。为什么需要编码呢？比如当前屏幕是1280720.一秒24张图片.那么我们一秒的视频数据是`1280720(位像素)*24(张) / 8(1字节8位)(结果:B) / 1024(结果:KB) / 1024 (结果:MB) = 2.64MB`一秒的数据有2.64MB数据量。1分钟就会有100多MB。这对用户来说真心是灾难。所以现在我们需要一种压缩方式减小数据的大小.在更低 比特率(bps)的情况下依然提供清晰的视频。 原理H.264原始码流(裸流)是由一个接一个NALU组成，它的功能分为两层，VCL(视频编码层)和 NAL(网络提取层).VCL(Video Coding Layer) + NAL(Network Abstraction Layer). VCL：包括核心压缩引擎和块，宏块和片的语法级别定义，设计目标是尽可能地独立于网络进行高效的编码； NAL：负责将VCL产生的比特字符串适配到各种各样的网络和多元环境中，覆盖了所有片级以上的语法级别。 在VCL进行数据传输或存储之前，这些编码的VCL数据，被映射或封装进NAL单元。（NALU）。一个NALU = 一组对应于视频编码的NALU头部信息 + 一个原始字节序列负荷(RBSP,Raw Byte Sequence Payload).如图所示，上图中的NALU的头 + RBSP 就相当于一个NALU(Nal Unit),每个单元都按独立的NALU传送。H.264的结构全部都是以NALU为主，理解了NALU，就理解了H.264的结构。一个原始的H.264 NALU 单元常由 [StartCode] [NALU Header] [NALU Payload] 三部分组成，其中 Start Code 用于标示这是一个NALU 单元的开始，必须是”00 00 00 01” 或”00 00 01” NAL Header由三部分组成，forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型）。 举例来说：00 00 00 01 06: SEI信息00 00 00 01 67: 0x67&amp;0x1f = 0x07 :SPS00 00 00 01 68: 0x68&amp;0x1f = 0x08 :PPS00 00 00 01 65: 0x65&amp;0x1f = 0x05: IDR Slice RBSPRBSP 序列举例RBSP 描述 SODB与RBSPSODB 数据比特串 -&gt; 是编码后的原始数据.RBSP 原始字节序列载荷 -&gt; 在原始编码数据的后面添加了 结尾比特。一个 bit“1”若干比特“0”，以便字节对齐。 从NALU出发了解H.264里面的专业词语 1帧 = n个片1片 = n个宏块1宏块 = 16x16yuv数据 Slice(片)如图所示，NALU的主体中包含了Slice(片).一个片 = Slice Header + Slice Data片是H.264提出的新概念，通过编码图片后切分通过高效的方式整合出来的概念。一张图片有一个或者多个片，而片由NALU装载并进行网络传输的。但是NALU不一定是切片，这是充分不必要条件，因为 NALU 还有可能装载着其他用作描述视频的信息.那么为什么要设置片呢?设置片的目的是为了限制误码的扩散和传输，应使编码片相互间是独立的。某片的预测不能以其他片中的宏块为参考图像，这样某一片中的预测误差才不会传播到其他片中。 可以看到上图中，每个图像中，若干宏块(Macroblock)被排列成片。一个视频图像可编程一个或更多个片，每片包含整数个宏块 (MB),每片至少包含一个宏块。片有一下五种类型: 片 意义 I 片 只包含I宏块 P 片 包含P和I宏块 B 片 包含B和I宏块 SP 片 包含P 和/或 I宏块,用于不同码流之间的切换 SI 片 一种特殊类型的编码宏块 宏块(Macroblock)刚才在片中提到了宏块.那么什么是宏块呢？宏块是视频信息的主要承载者。一个编码图像通常划分为多个宏块组成.包含着每一个像素的亮度和色度信息。视频解码最主要的工作则是提供高效的方式从码流中获得宏块中像素阵列。一个宏块 = 一个16*16的亮度像素 + 一个8×8Cb + 一个8×8Cr彩色像素块组成。(YCbCr 是属于 YUV 家族的一员,在YCbCr 中 Y 是指亮度分量，Cb 指蓝色色度分量，而 Cr 指红色色度分量) 宏块分类 意义 I 宏块 利用从当前片中已解码的像素作为参考进行帧内预测 P 宏块 利用前面已编码图像作为参考进行帧内预测，一个帧内编码的宏块可进一步作宏块的分割:即16×16.16×8.8×16.8×8亮度像素块。如果选了8×8的子宏块，则可再分成各种子宏块的分割，其尺寸为8×8，8×4，4×8，4×4 B 宏块 利用双向的参考图像(当前和未来的已编码图像帧)进行帧内预测 在 H.264 中，句法元素共被组织成 序列、图像、片、宏块、子宏块五个层次。句法元素的分层结构有助于更有效地节省码流。例如，再一个图像中，经常会在各个片之间有相同的数据，如果每个片都同时携带这些数据，势必会造成码流的浪费。更为有效的做法是将该图像的公共信息抽取出来，形成图像一级的句法元素，而在片级只携带该片自身独有的句法元素。 宏块分类 意义 mb_type 确定该 MB 是帧内或帧间(P 或 B)编码模式，确定该 MB 分割的尺寸 mb_pred 确定帧内预测模式(帧内宏块)确定表 0 或表 1 参考图 像，和每一宏块分割的差分编码的运动矢量(帧间宏块，除 8×8 宏块分割的帧内 MB) sub_mb_pred (只对 8×8MB 分割的帧内 MB)确定每一子宏块的子宏 块分割，每一宏块分割的表 0 和/或表 1 的参考图象;每一 宏块子分割的差分编码运动矢量。 coded_block_pattern 指出哪个 8×8 块(亮度和彩色)包 编码变换系数 mb_qp_delta 量化参数的改变值 residual 预测后对应于残差图象取样的编码变换系数 图像,场和帧图像是个集合概念，顶 场、底场、帧都可以称为图像。对于H.264 协议来说，我们平常所熟悉的那些称呼，例如： I 帧、P 帧、B帧等等，实际上都是我们把图像这个概念具体化和细小化了。我们 在 H.264里提到的“帧”通常就是指不分场的图像； 视频的一场或一帧可用来产生一个编码图像。一帧通常是一个完整的图像。当采集视频信号时，如果采用隔行扫描(奇.偶数行),则扫描下来的一帧图像就被分为了两个部分,这每一部分就被称为[场],根据次序氛围: [顶场] 和 [底场]。 方式 作用域 帧编码方式 活动量较小或者静止的图像宜采用 场编码方式 活动量较大的运动图像 I,P,B帧与pts/dts 帧的分类 中文 意义 I帧 帧内编码帧,又称intra picture I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩，做为随机访问的参考点，可以当成图象。I帧可以看成是一个图像经过压缩后的产物 P帧 前向预测编码帧,又称predictive-frame 通过充分将低于图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧 B帧 双向预测帧,又称bi-directional interpolated prediction frame 既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像,也叫双向预测帧 I P B帧的不同: I frame:自身可以通过视频解压算法解压成一张单独的完整的图片。 P frame：需要参考其前面的一个I frame 或者B frame来生成一张完整的图片。 B frame:则要参考其前一个I或者P帧及其后面的一个P帧来生成一张完整的图片。 名称 | 意义PTS(Presentation Time Stamp) | PTS主要用于度量解码后的视频帧什么时候被显示出来。DTS(Decode Time Stamp) | DTS主要是标识内存中的bit流再什么时候开始送入解码器中进行解码。 DTS与PTS的不同:DTS主要用户视频的解码，在解码阶段使用。PTS主要用于视频的同步和输出，在display的时候使用。再没有B frame的时候输出顺序一样。 GOPGOP是画面组，一个GOP是一组连续的画面。GOP一般有两个数字，如M=3，N=12.M制定I帧与P帧之间的距离，N指定两个I帧之间的距离。那么现在的GOP结构是I BBP BBP BBP BB I增大图片组能有效的减少编码后的视频体积，但是也会降低视频质量，至于怎么取舍，得看需求了 IDR一个序列的第一个图像叫做 IDR 图像（立即刷新图像），IDR 图像都是 I 帧图像。I和IDR帧都使用帧内预测。I帧不用参考任何帧，但是之后的P帧和B帧是有可能参考这个I帧之前的帧的。IDR就不允许这样。比如这种情况:IDR1 P4 B2 B3 P7 B5 B6 I10 B8 B9 P13 B11 B12 P16 B14 B15 这里的B8可以跨过I10去参考P7 核心作用：H.264 引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码。 帧内预测和帧间预测帧内预测（也叫帧内压缩） 我们可以通过第 1、2、3、4、5 块的编码来推测和计算第 6 块的编码，因此就不需要对第 6 块进行编码了，从而压缩了第 6 块，节省了空间 帧间预测（也叫帧间压缩） 可以看到前后两帧的差异其实是很小的，这时候用帧间压缩就很有意义。这里涉及到几个重要的概念：块匹配，残差，运动搜索(运动估计),运动补偿. 帧间压缩最常用的方式就是块匹配(Block Matching)。找找看前面已经编码的几帧里面，和我当前这个块最类似的一个块，这样我不用编码当前块的内容了，只需要编码当前块和我找到的快的差异(残差)即可。找最想的块的过程叫运动搜索(Motion Search),又叫运动估计。用残差和原来的块就能推算出当前块的过程叫运动补偿(Motion Compensation). 参考链接新一代视频压缩编码标准H.264 关于视频的一些概念I,P，B帧和PTS，DTS的关系]]></content>
      <tags>
        <tag>h264</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视音频编解码技术零基础学习方法]]></title>
    <url>%2F2019%2F03%2F06%2F%E8%A7%86%E9%9F%B3%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81%E6%8A%80%E6%9C%AF%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[天妒英才，不夸张的说，如果不知道雷霄骅，可能你音视频还没入门.借用新浪网友 @张新成010 的话。“你的博客纯粹是为了分享，写的很仔细，引领了多少人入门，在音视频方面有自己的见解，在当今音视频编解码封闭技术领域，你摒弃了别人的躲躲藏藏，无私奉献，你是伟大的，很多音视频方向的朋友称你为雷神，你无愧于这个称号！一个在大学里能静下心来做研究，又能无私分享的，尤其是在中国的大学里，你是难能可贵的佼佼者，至此以后，博客上不会再有更新，音视频的世界少了一颗将才。我已无语，痛心而疾，雷霄骅，雷神，感谢有你，天堂安好！”传送门]]></content>
      <tags>
        <tag>音视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 入门参考]]></title>
    <url>%2F2019%2F03%2F06%2FMarkdown-%E5%85%A5%E9%97%A8%E5%8F%82%E8%80%83%2F</url>
    <content type="text"><![CDATA[Markdown是一种轻量级标记语言，创始人为约翰·格鲁伯（英语：John Gruber）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML（或者HTML）文档”。[4]这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。 由于Markdown的轻量化、易读易写特性，并且对于图片，图表、数学式都有支持，当前许多网站都广泛使用 Markdown 来撰写帮助文档或是用于论坛上发表消息。例如：GitHub、reddit、Diaspora、Stack Exchange、OpenStreetMap 、SourceForge等。甚至Markdown能被使用来撰写电子书。传送门]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homebrew Bottles源]]></title>
    <url>%2F2019%2F03%2F02%2FHomebrew-Bottles%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Homebrew Bottles源Homebrew Bottles是Homebrew提供的二进制代码包，目前镜像站收录了以下仓库： homebrew/homebrew-core homebrew/homebrew-dupes homebrew/homebrew-games homebrew/homebrew-gui homebrew/homebrew-python homebrew/homebrew-php homebrew/homebrew-science homebrew/homebrew-versions homebrew/homebrew-x11 使用方法对于bash用户：12echo &apos;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles&apos; &gt;&gt; ~/.bash_profilesource ~/.bash_profile 对于zsh用户12echo &apos;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles&apos; &gt;&gt; ~/.zshrcsource ~/.zshrc 更多说明]]></content>
      <tags>
        <tag>homebrew</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换及重置Homebrew默认源]]></title>
    <url>%2F2019%2F03%2F02%2F%E6%9B%BF%E6%8D%A2%E5%8F%8A%E9%87%8D%E7%BD%AEHomebrew%E9%BB%98%E8%AE%A4%E6%BA%90%2F</url>
    <content type="text"><![CDATA[====== 替换及重置Homebrew默认源 ======替换brew.git:cd “$(brew —repo)”git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git&lt;/code&gt; 替换Homebrew Bottles源:参考:[[https://lug.ustc.edu.cn/wiki/mirrors/help/homebrew-bottles|替换Homebrew Bottles源]] 在中科大源失效或宕机时可以： 使用[[https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/|清华源设置参考]]。 切换回官方源：重置brew.git:cd “$(brew —repo)”git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://github.com/Homebrew/homebrew-core.git&lt;/code&gt; 注释掉bash配置文件里的有关Homebrew Bottles即可恢复官方源。重启bash或让bash重读配置文件。]]></content>
      <tags>
        <tag>homebrew</tag>
      </tags>
  </entry>
</search>
