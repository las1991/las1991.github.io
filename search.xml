<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2020%2F12%2F17%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[分布式锁用来锁什么？ 效率(efficiency)，避免不必要的重复昂贵计算。如果锁失败了，我们可能会面临两个节点执行相同的任务，会造成提升成本（为aws多支付5美分）或者一些小麻烦（比如给用户发送重复的邮件等） 正确性(correctness)，防止并发问题和保证系统正确状态。如果失败了，两个节点使用相同的数据并行工作，会造成文件损坏，数据丢失，数据不一致，例如给病人服用错误的药量、奖品超发等一系列严重问题 分布式锁需要满足哪些？安全属性：相互排斥。任何时候，只有1个client可以持有锁可用属性A：死锁释放。在客户端锁住资源之后发生宕机或网络不通时，锁应该可以达到被获取的状态可用属性B：容错性。只要大多数redis节点存活，客户端应该可以申请、释放锁。 redis单实例/单集群实现 申请锁： 1SET resource_name my_random_value NX PX 30000 释放锁： 12345if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end redis单实例/单集群的问题 单实例时会有单点故障 主从时，redis主从采用异步复制数据 client A 从master获取锁 master宕机，锁数据未同步到slave slave 升级为master client B 获取锁，此时A还未释放，此时违反了相互排斥原则 The Redlock algorithm我们假设有N个redis master节点，这些节点是独立的，他们之间不存在数据复制或其他任何隐式系统的协调。假设N=5。 client获取锁有以下操作： 获取当前时间毫秒值 使用相同的key，随机的value，尝试在所有实例上连续的获取锁，在第二步为了获取锁，client超时时间小于锁自动释放时间，例如自动释放时间为10秒，超时时间应该在5-50ms之间。防止客户端与宕机节点阻塞很长时间，如果一台实例不可用，我们应该尽快的与下一个实例交互。 client根据第一步获取的时间，计算消耗的时间，以重新获取锁，仅当client在多数实例获取锁成功（至少3个），并且消耗时间小于锁的过期时间，锁才会被认为获取成功。 如果锁获取了锁，锁的有效时间是第三步计算时间与初始化有效时间的较小值。 如果client因为某些原因获取失败（没有达到N/2+1个实例或超过有效期)，那么将在所有实例释放锁（即使实例被认为未能获取锁） redlock的问题 在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的lease这个词可以暂且认为就等同于一个带有自动过期功能的锁。客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。 初看上去，有人可能会说，既然客户端1从GC pause中恢复过来以后不知道自己持有的锁已经过期了，那么它可以在访问共享资源之前先判断一下锁是否过期。但仔细想想，这丝毫也没有帮助。因为GC pause可能发生在任意时刻，也许恰好在判断完之后。 也有人会说，如果客户端使用没有GC的语言来实现，是不是就没有这个问题呢？Martin指出，系统环境太复杂，仍然有很多原因导致进程的pause，比如虚存造成的缺页故障(page fault)，再比如CPU资源的竞争。即使不考虑进程pause的情况，网络延迟也仍然会造成类似的结果。 总结起来就是说，即使锁服务本身是没有问题的，而仅仅是客户端有长时间的pause或网络延迟，仍然会造成两个客户端同时访问共享资源的冲突情况发生。 Redlock对系统计时(timing)的过分依赖Martin在文中构造了一些事件序列，能够让Redlock失效（两个客户端同时持有锁），假设有5个Redis节点A, B, C, D, E： 客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。 节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。 客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。 客户端1和客户端2现在都认为自己持有了锁。 上面这种情况之所以有可能发生，本质上是因为Redlock的安全性(safety property)对系统的时钟有比较强的依赖，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。Martin在这里其实是要指出分布式算法研究中的一些基础性问题，或者说一些常识问题，即好的分布式算法应该基于异步模型(asynchronous model)，算法的安全性不应该依赖于任何记时假设(timing assumption)。在异步模型中：进程可能pause任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。一个好的分布式算法，这些因素不应该影响它的安全性(safety property)，只可能影响到它的活性(liveness property)，也就是说，即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。这样的算法在现实中是存在的，像比较著名的Paxos，或Raft。但显然按这个标准的话，Redlock的安全性级别是达不到的。 客户端GC pause引发Redlock失效 客户端1向Redis节点A, B, C, D, E发起锁请求。 各个Redis节点已经把请求结果返回给了客户端1，但客户端1在收到请求结果之后进入了长时间的GC pause。 在所有的Redis节点上，锁过期了。 客户端2在A, B, C, D, E上获取到了锁。 客户端1从GC pause从恢复，客户端1认为自己成功获取到了锁，继续处理业务 客户端1和客户端2现在都认为自己持有了锁。 Martin给出的这个例子其实有点小问题。在Redlock算法中，客户端在完成向各个Redis节点的获取锁的请求之后，会计算这个过程消耗的时间，然后检查是不是超过了锁的有效时间(lock validity time)。也就是上面的例子中第5步，客户端1从GC pause中恢复过来以后，它会通过这个检查发现锁已经过期了，不会再认为自己成功获取到锁了。我对Martin的例子做了简单的调整：gc pause节点，由收到回复之前，改到确认加锁成功之后、执行业务之前。antirez在他的反驳文章中就指出来了这个问题，但Martin认为这个细节对Redlock整体的安全性没有本质的影响。 Martin构造这里的这个例子，是为了强调在一个分布式的异步环境下，长时间的GC pause或消息延迟（上面这个例子中，把GC pause换成Redis节点和客户端1之间的消息延迟，逻辑不变），会让客户端获得一个已经过期的锁。从客户端1的角度看，Redlock的安全性被打破了，因为客户端1收到锁的时候，这个锁已经失效了，而Redlock同时还把这个锁分配给了客户端2。换句话说，Redis服务器在把锁分发给客户端的途中，锁就过期了，但又没有有效的机制让客户端明确知道这个问题。而在之前的那个例子中，客户端1收到锁的时候锁还是有效的，锁服务本身的安全性可以认为没有被打破，后面虽然也出了问题，但问题是出在客户端1和共享资源服务器之间的交互上。 zookeeper实现 客户端尝试创建一个znode节点，比如/lock。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode已存在），获取锁失败。 持有锁的客户端访问共享资源完成后，将znode删掉，这样其它客户端接下来就能来获取锁了。 znode应该被创建成ephemeral的。这是znode的一个特性，它保证如果创建znode的那个客户端崩溃了，那么相应的znode会被自动删除。这保证了锁一定会被释放。 zookeeper实现的问题ZooKeeper是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与ZooKeeper的某台服务器维护着一个Session，这个Session依赖定期的心跳(heartbeat)来维持。如果ZooKeeper长时间收不到客户端的心跳（这个时间称为Sesion的过期时间），那么它就认为Session过期了，通过这个Session所创建的所有的ephemeral类型的znode节点都会被自动删除。 设想如下的执行序列： 客户端1创建了znode节点/lock，获得了锁。 客户端1进入了长时间的GC pause。 客户端1连接到ZooKeeper的Session过期了。znode节点/lock被自动删除。 客户端2创建了znode节点/lock，从而获得了锁。 客户端1从GC pause中恢复过来，它仍然认为自己持有锁。 最后，客户端1和客户端2都认为自己持有了锁，冲突了。看起来，用ZooKeeper实现的分布式锁也不一定就是安全的。该有的问题它还是有。但是，ZooKeeper作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，是Redis之类的方案所没有的。像前面提到的ephemeral类型的znode自动删除的功能就是一个例子。 还有一个很有用的特性是ZooKeeper的watch机制。这个机制可以这样来使用，比如当客户端试图创建/lock的时候，发现它已经存在了，这时候创建失败，但客户端不一定就此对外宣告获取锁失败。客户端可以进入一种等待状态，等待当/lock节点被删除的时候，ZooKeeper通过watch机制通知它，这样它就可以继续完成创建操作（获取锁）。这可以让分布式锁在客户端用起来就像一个本地的锁一样：加锁失败就阻塞住，直到获取到锁为止。这样的特性Redlock就无法实现。 顺便提一下，如上所述的基于ZooKeeper的分布式锁的实现，并不是最优的。它会引发“herd effect”（羊群效应），降低获取锁的性能。一个更好的实现参见下面链接： http://zookeeper.apache.org/doc/r3.4.9/recipes.html#sc_recipes_Locks 如何解决？Martin给出了一种方法，称为fencing token。fencing token是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个fencing token，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。如下图： fencing token基于random token的“Check and Set” 先设置X.currlock = token。 读出资源X（包括它的值和附带的X.currlock）。 按照”write-if-currlock == token”的逻辑，修改资源X的值。意思是说，如果对X进行修改的时候，X.currlock仍然和当初设置进去的token相等，那么才进行修改；如果这时X.currlock已经是其它值了，那么说明有另外一方也在试图进行修改操作，那么放弃当前的修改，从而避免冲突。 antirez举了一个“将名字加入列表”的操作的例子： T0: Client A receives new name to add from web. T0: Client B is idle T1: Client A is experiencing pauses. T1: Client B receives new name to add from web. T2: Client A is experiencing pauses. T2: Client B receives a lock with ID 1 T3: Client A receives a lock with ID 2 “Check and Set”对于写操作要分成两步来完成（设置token、判断-写回），而递增的fencing token机制只需要一步（带着token向资源服务器发起写请求）。 递增的fencing token机制能保证最终操作共享资源的顺序，那些延迟时间太长的操作就无法操作共享资源了。但是基于random token的“Check and Set”操作不会保证这个顺序，那些延迟时间太长的操作如果后到达了，它仍然有可能操作共享资源（当然是以互斥的方式）。 fencing token 实现Chubby的分布式锁是怎样做fencing的？提到分布式锁，就不能不提Google的Chubby。 Chubby是Google内部使用的分布式锁服务，有点类似于ZooKeeper，但也存在很多差异。Chubby对外公开的资料，主要是一篇论文，叫做“The Chubby lock service for loosely-coupled distributed systems”，下载地址如下： https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/chubby-osdi06.pdf http://systemdesigns.blogspot.com/2016/01/chubby-lock-service_10.html 另外，YouTube上有一个的讲Chubby的talk，也很不错，播放地址： https://www.youtube.com/watch?v=PqItueBaiRg&amp;feature=youtu.be&amp;t=487 Chubby自然也考虑到了延迟造成的锁失效的问题。论文里有一段描述如下： a process holding a lock L may issue a request R, but then fail. Another process may ac- quire L and perform some action before R arrives at its destination. If R later arrives, it may be acted on without the protection of L, and potentially on inconsistent data.（译文： 一个进程持有锁L，发起了请求R，但是请求失败了。另一个进程获得了锁L并在请求R到达目的方之前执行了一些动作。如果后来请求R到达了，它就有可能在没有锁L保护的情况下进行操作，带来数据不一致的潜在风险。） Chubby给出的用于解决（缓解）这一问题的机制称为sequencer，类似于fencing token机制。锁的持有者可以随时请求一个sequencer，这是一个字节串，它由三部分组成： 锁的名字。锁的获取模式（排他锁还是共享锁）。lock generation number（一个64bit的单调递增数字）。作用相当于fencing token或epoch number。客户端拿到sequencer之后，在操作资源的时候把它传给资源服务器。然后，资源服务器负责对sequencer的有效性进行检查。检查可以有两种方式： 调用Chubby提供的API，CheckSequencer()，将整个sequencer传进去进行检查。这个检查是为了保证客户端持有的锁在进行资源访问的时候仍然有效。将客户端传来的sequencer与资源服务器当前观察到的最新的sequencer进行对比检查。可以理解为与Martin描述的对于fencing token的检查类似。当然，如果由于兼容的原因，资源服务本身不容易修改，那么Chubby还提供了一种机制： lock-delay。Chubby允许客户端为持有的锁指定一个lock-delay的时间值（默认是1分钟）。当Chubby发现客户端被动失去联系的时候，并不会立即释放锁，而是会在lock-delay指定的时间内阻止其它客户端获得这个锁。这是为了在把锁分配给新的客户端之前，让之前持有锁的客户端有充分的时间把请求队列排空(draining the queue)，尽量防止出现延迟到达的未处理请求。可见，为了应对锁失效问题，Chubby提供的三种处理方式：CheckSequencer()检查、与上次最新的sequencer对比、lock-delay，它们对于安全性的保证是从强到弱的。而且，这些处理方式本身都没有保证提供绝对的正确性(correctness)。但是，Chubby确实提供了单调递增的lock generation number，这就允许资源服务器在需要的时候，利用它提供更强的安全性保障。 总结无论redis还是zookeeper实现的分布式锁，都是在客户端实现，此时我们把获取锁的线程当作客户端，访问的资源作为服务端。 多个或同一个客户端做一件事，比如扫码支付/抢一个商品/提交同一个form表单，最终只能有一个可以成功，在客户端我们只能做一些辅助性的拦截（防止重复提交，库存校验等），无论客户端做不做这些，我们服务端都要再次校验。 在分布式环境下，会有网络，时钟，gc各种原因会造成客户端的到达服务端的顺序不一致，所以，无论有没有分布式锁，我们都要在资源（服务端）进行校验，从而保证系统正确性。 我觉得，分布式锁有两个作用： 限流，降低访问共享资源的并发 分配访问共享资源的ticket 至于这个ticket是有效还是无效，最终要靠共享资源（服务端）决定。 参考Distributed locks with RedisHow to do distributed locking针对Martin的blog的讨论针对antirez的blog的讨论Note on fencing and distributed locksThe Chubby lock service for loosely-coupled distributed systems基于Redis的分布式锁到底安全吗（上）？基于Redis的分布式锁到底安全吗（下）？]]></content>
      <tags>
        <tag>redis</tag>
        <tag>分布式</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群脑裂分析]]></title>
    <url>%2F2020%2F12%2F11%2Fredis%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[哨兵(sentinel)模式下的脑裂 如上图，1个master与3个slave组成的哨兵模式（哨兵独立部署于其它机器），刚开始时，2个应用服务器server1、server2都连接在master上，如果master与slave及哨兵之间的网络发生故障，但是哨兵与slave之间通讯正常，这时3个slave其中1个经过哨兵投票后，提升为新master，如果恰好此时server1仍然连接的是旧的master，而server2连接到了新的master上。 数据就不一致了，基于setNX指令的分布式锁，可能会拿到相同的锁；基于incr生成的全局唯一id，也可能出现重复。 解决方案： 1.使用如下配置1234# 连接到master的最少slave数min-slaves-to-write 1# slave连接master的最大延迟时间min-slaves-max-lag 10 10秒内的数据会混乱， 集群(cluster)模式下的脑裂 sentinelcluster-tutorial]]></content>
      <tags>
        <tag>redis</tag>
        <tag>分布式</tag>
        <tag>高可用</tag>
        <tag>脑裂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javaWeb三大组件]]></title>
    <url>%2F2020%2F11%2F14%2FjavaWeb%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Filter与Listener区别 Servlet Filter用来监听（monitoring）从客户端到servlet的request、response，或者修改request、response，或者鉴权、记录日志 Servlet Listener用来收听web容器的事件，例如session生命周期变化、session属性值变化、Servlet context生命周期变化、Servlet context属性值变化。 Filter is just like a water filter, where incoming (request) and outgoing (response) values will be filtered. Listener is like listening (trigger) - whenever required, I will be performed. Difference between Filter and Listener in Servlet (Java EE) Servlet Filters and Event Listeners Servlet Servlet详解 Spring MVC DispatcherServlet 1. 首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制； 2. DispatcherServlet——&gt;HandlerMapping， HandlerMapping将会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象、多个HandlerInterceptor拦截器）对象，通过这种策略模式，很容易添加新的映射策略； 3. DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器； 4. HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView对象（包含模型数据、逻辑视图名）； 5. ModelAndView的逻辑视图名——&gt; ViewResolver， ViewResolver将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术； 6. View——&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术； 7. 返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。 DispatcherServlet请求处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; if (logger.isDebugEnabled()) &#123; String resumed = WebAsyncUtils.getAsyncManager(request).hasConcurrentResult() ? &quot; resumed&quot; : &quot;&quot;; logger.debug(&quot;DispatcherServlet with name &apos;&quot; + getServletName() + &quot;&apos;&quot; + resumed + &quot; processing &quot; + request.getMethod() + &quot; request for [&quot; + getRequestUri(request) + &quot;]&quot;); &#125; // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;String, Object&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(&quot;org.springframework.web.servlet&quot;)) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125;&#125; doDispatch()方法源码：该过程是负责请求的真正处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; // 1. 判断用户的请求是否为文件上传，通过MultipartResolver解析 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 2. 通过HandlerMapping映射处理器和拦截器包装成一个HandlerExecutionChain mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // 3. 将处理器包装成相应的适配器 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 省略此处代码 // 4. 执行处理器相关的拦截器的预处理 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // 5. 由适配器执行处理器 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(request, mv); // 6. 拦截器后处理 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; // 7. 视图渲染 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; // 此处省略部分代码&#125;]]></content>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 进阶]]></title>
    <url>%2F2020%2F11%2F04%2FMysql-%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[参考MySQL体系架构简介MySQL连接机制浅析及运维MySQL用户及权限MySQL缓存机制MySQL查询优化器SQL教程及练习题InnoDB存储引擎架构简介MySQL日志分类及简介MySQL事务ACID详解InnoDB Buffer Pool详解MySQL InnoDB文件存储结构]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mysql 优化3板斧]]></title>
    <url>%2F2020%2F11%2F04%2FMysql-%E4%BC%98%E5%8C%963%E6%9D%BF%E6%96%A7%2F</url>
    <content type="text"><![CDATA[explainshow profileshow processlist参考explain、show profile和show processlist]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS之SNI]]></title>
    <url>%2F2020%2F10%2F28%2FHTTPS%E4%B9%8BSNI%2F</url>
    <content type="text"><![CDATA[SNISNI（Server Name Indication）是 TLS 的扩展，用来解决一个服务器拥有多个域名的情况。 在客户端和服务端建立 HTTPS 的过程中要先进行 TLS 握手，握手后会将 HTTP 报文使用协商好的密钥加密传输。 在 TLS 握手信息中并没有携带客户端要访问的目标地址。这样会导致一个问题，如果一台服务器有多个虚拟主机，且每个主机的域名不一样，使用了不一样的证书，该和哪台虚拟主机进行通信？ 和 HTTP 协议用来解决服务器多域名的方案类似，HTTP 在请求头中使用 Host 字段来指定要访问的域名。TLS 的做法，也是加 Host，在 TLS 握手第一阶段 ClientHello 的报文中添加。 SNI 在 TLSv1.2 开始得到支持。从 OpenSSL 0.9.8 版本开始支持。所以基本市场上的终端设备都支持。 使用 WireShark 抓包看一下 ClientHello： 可以看到包中的 SNI 扩展字段：12345678Extension: server_name Type: server_name (0x0000) Length: 19 Server Name Indication extension Server Name list length: 17 Server Name Type: host_name (0) Server Name length: 14 Server Name: www.github.com 这里指定了该 TLS 握手的目标域名为 www.github.com 。通过 SNI，拥有多虚拟机主机和多域名的服务器就可以正常建立 TLS 连接了。 nginx支持TLS协议的SNI扩展nginx手册 JDK支持SNI扩展默认为true System.setProperty( “jsse.enableSNIExtension”, “true” ); jdk关于SNI支持的bug1.8.141版本修复了 Custom HostnameVerifier disables SNI extension]]></content>
      <tags>
        <tag>HTTPS</tag>
        <tag>TLS</tag>
        <tag>SNI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql Explain 解释]]></title>
    <url>%2F2020%2F10%2F28%2FMysql-Explain-%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[列名 类型 解释 id SELECT语句的ID编号,优先执行编号较大的查询,如果编号相同,则从上向下执行 select_type SIMPLE 一条没有UNION或子查询部分的SELECT语句 PIMARY 最外层或最左侧的SELECT语句 UNION UNION语句里的第二条或最后一条SELECT语句 DEPENDENT UNION 和UNION类型的含义相似,但需要依赖于某个外层查询 UNION RESULT 一条UNION语句的结果 SUBQUERY 子查询中的第一个SELECT子句 DEPENDENT SUBQUERY 和SUBQUERY类型的含义相似,但需要依赖于某个外层查询 DERIVED FROM子句里的子查询 table t1 各输出行里的信息是关于哪个数据表的 Partitions NULL 将要使用的分区.只有EXPLAIN PARTITIONS …语句才会显示这一列.非分区表显示为NULL type 联接操作的类型,性能由好到差依次如下 system 表中仅有一行 const 单表中最多有一个匹配行 eq_ref 联接查询中,对于前表的每一行,在此表中只查询一条记录,使用了PRIMARY或UNIQUE ref 联接查询中,对于前表的每一行,在此表中只查询一条记录,使用了INDEX ref_or_null 联接查询中,对于前表的每一行,在此表中只查询一条记录,使用了INDEX,但是条件中有NULL值查询 index_merge 多个索引合并 unique_subquery 举例说明: value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery 举例说明: value IN (SELECT key_column FROM single_table WHERE some_expr) range 只检索给定范围的行,包括如下操作符: =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, or IN() index 扫描索引树(略比ALL快,因为索引文件通常比数据文件小) ALL 前表的每一行数据都要跟此表匹配,全表扫描 possible_keys NULL MySQL认为在可能会用到的索引.NULL表示没有找到索引 key NULL 检索时,实际用到的索引名称.如果用了index_merge联接类型,此时会列出多个索引名称,NULL表示没有找到索引 key_len NULL 实际使用的索引的长度.如果是复合索引,那么只显示使用的最左前缀的大小 ref NULL MySQL用来与索引值比较的值, 如果是单词const或者???,则表示比较对象是一个常数.如果是某个数据列的名称,则表示比较操作是逐个数据列进行的.NULL表示没有使用索引 rows MySQL为完成查询而需要在数据表里检查的行数的估算值.这个输出列里所有的值的乘积就是必须检查的数据行的各种可能组合的估算值 Extra Using filesort 需要将索引值写到文件中并且排序,这样按顺序检索相关数据行 Using index MySQL可以不必检查数据文件, 只使用索引信息就能检索数据表信息 Using temporary 在使用 GROUP BY 或 ORDER BY 时,需要创建临时表,保存中间结果集 Using where 利用SELECT语句中的WHERE子句里的条件进行检索操作 参考explain详解]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何画好架构图？]]></title>
    <url>%2F2020%2F10%2F21%2F%E5%A6%82%E4%BD%95%E7%94%BB%E5%A5%BD%E6%9E%B6%E6%9E%84%E5%9B%BE%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[首先，我们应该明确一点，应该基于领域来划分架构的边界，每一篇架构图都是一个独立的领域。那么领域该如何划分呢？架构图又应该包含哪些方面呢？ 术语领域划分、边界 领域他不是部门，比如C端不是一个领域而是一个组织，一个组织可以有很多个领域。举个简单的例子，一个C端的订单详情页，可能需要类似导购、交易、库存、价格、商品、营销多个领域的聚合。一个领域应该是核心的业务问题域，他自身的特点应该是高内聚、边界性强、操作关联性强、有自己的独立实体。 子领域 相对领域而言，子领域的概念可以理解为领域的子集，如交易中正向交易、逆向交易等。 模块子领域应该由多个模块组成，模块是用户能完成一个业务目标的最小颗粒度的完整功能，比如展示可以购买商品的列表页、购物车、下单等，也可以是一个通用的模块，比如超时取消订单、流程编排等。 架构图那么说完基本的领域概念和术语，接下来阐述一下架构图的几个分类和画法。 业务架构业务架构或者说业务领域图，他的作用是来描述功能点和业务流程，受众可以是产品、运营、技术等。 业务架构是根据不通的功能模块来组织其相互之间的关系的，一个产品中不同的功能模块之间的关系分直接关系和间接关系，只有直接关系的功能模块才会被组织到一起形成一个子系统，当有直接关系的功能模块组成一个子系统之后，解决相同问题域的子系统就形成一个功能层级。功能层级按照最接近用户实际操作的距离程度从上到下或者从左到右的划分，就形成了产品架构的分层。 系统架构系统架构主要用来描述系统由哪些领域和子领域的功能模块组成，一般针对开发人员、构架师、产品等角色。 如果说业务架构、产品架构是站在业务和产品的角度来看待领域和模块的关系，那么系统架构就是站在技术实现的角度按照不同的子领域的功能模块来阐述系统的组成。 子领域架构全景图关键业务场景图关键路径流程图部署图参考如何画好架构图？]]></content>
  </entry>
  <entry>
    <title><![CDATA[谷歌Jeff Dean阐述分布式系统设计模式]]></title>
    <url>%2F2020%2F10%2F19%2F%E8%B0%B7%E6%AD%8CJeff-Dean%E9%98%90%E8%BF%B0%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[PPT链接PPT链接 分布式系统设计模式 系统失败是很平常的事情：每年有1-5%的硬盘会报废，服务器每年会平均宕机两次，报废几率在2-4%几率。 将一个大而复杂系统切分为多个服务：而且服务之间依赖尽可能的少，这样有助于测试，部署和小团队独立开发。例子：一个google的搜索会依赖100多个服务。吴注：需要一套机制来确保服务的fault-tolerant，不能让一个服务的成败影响全局。 需要有Protocol Description Language：比如protocol buffers。吴注：这样能降低通信方面的代码量。 有能力在开发之前，根据系统的设计来预测性能：在最下面有一些重要的数字。 在设计系统方面，不要想做的很全面，而是需要抓住重点。 为了增量做设计，但不为无限做设计。比如：要为5-50倍的增量做设计，但超过1000倍了，就需要重写和重新设计了。 使用备份请求来降低延迟：比如一个处理需要涉及1000台机器，通过备份请求这个机制来避免这个处理被一台慢机器延误。吴注：这个机制非常适合MapReduce。 使用范围来分布数据，而不是Hash：因为这样在语义上比较简单，并且容易控制。吴注：在大多数情况下语义比性能更重要，不要为了20%的情况hardcode。 灵活的系统，根据需求来伸缩：并且当需求上来的时候，关闭部分特性，比如：关闭拼写检查。 一个接口，多个实现。 加入足够的观察和调式钩子（hook）。 1000台服务器只需单一Master：通过Master节点来统一指挥全部的行动，但客户端和Master节点的交互很少，以免Master节点Crash，优点是，在语义上面非常清晰，但伸缩性不是非常强，一般最多只能支持上千个节点。 在一台机器上运行多个单位的服务：当一台机器宕机时，能缩短相应的恢复时间，并且支持细粒度的负载均衡，比如在BigTable中，一个Tablet服务器会运行多个Tablet。 未来的挑战 全球级（world-wide）系统的适应性方面：如何自动地分配和放置数据和计算来降低延迟和成本。 在弱一致性（weakly consistent）的存储上搭建应用：如何轻松使用抽象来解决多版本之间的冲突。 分布式系统的抽象：如何用同一个抽象来统一多个分布式系统。 关于JeffreyJeffrey Dean Google Fellow。2009年当选美国工程院院士。他是Google公司最具才华的工程师之一。众多Google的核心产品都有他的重大贡献，包括设计和实现了Google广告服务系统的最初版本，Google爬虫、索引和查询服务系统的五个版本，Adsense最初版本，Protocol Buffers，Google News的服务系统，MapReduce，BigTable，等等。当然，也包括搜索排序算法的许多方面，Google Translate和Google Code Search的部分开发。 1996年获得华盛顿大学计算机科学博士学位，课题是面向对象语言中的全程序优化技术。1990年毕业于明尼苏达大学，获得计算机科学和经济学学士学位。1999年加入Google之前，曾效力于DEC研究中心。本科毕业后曾经在世界卫生组织艾滋病项目工作，开发了艾滋病传染的统计建模、预测和分析软件。 他的一个人生目标是在所有大洲打篮球和橄榄球。]]></content>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cpu内存访问速度，磁盘和网络速度，所有人都应该知道的数字]]></title>
    <url>%2F2020%2F10%2F19%2F%E6%89%80%E6%9C%89%E4%BA%BA%E9%83%BD%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84%E6%95%B0%E5%AD%97%2C%E7%B3%BB%E7%BB%9F%E6%93%8D%E4%BD%9C%E8%80%97%E6%97%B6%2F</url>
    <content type="text"><![CDATA[所有人都应该知道的数字,系统操作耗时google 工程师Jeff Dean 首先在他关于分布式系统的ppt文档列出来的，到处被引用的很多。1纳秒等于10亿分之一秒，= 10 ^ -9 秒 Numbers Everyone Should Know 操作内容 时间 L1 cache reference 读取CPU的一级缓存 0.5ns Branch mispredict(转移、分支预测) 5ns L2 cache reference 读取CPU的二级缓存 7ns Mutex lock/unlock 互斥锁\解锁 100ns Main memory reference 读取内存数据 100ns Compress 1K bytes with Zippy 1k字节压缩 10,000ns Send 2K bytes over 1 Gbps network 在1Gbps的网络上发送2k字节 20,000ns Read 1 MB sequentially from memory 从内存顺序读取1MB 250,000 ns=0.25ms Round trip within same datacenter 从一个数据中心往返一次，ping一下 500,000ns=0.5ms Disk seek 磁盘搜索 10,000,000ns=10ms Read 1 MB sequentially from network 从网络上顺序读取1兆的数据 10,000,000ns=10ms Read 1 MB sequentially from disk 从磁盘里面读出1MB 30,000,000ns=30ms Send packet CA-&gt;Netherlands-&gt;CA 一个包的一次远程访问 150,000,000ns=150ms]]></content>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寄存器比内存访问速度快的原因]]></title>
    <url>%2F2020%2F10%2F17%2F%E5%AF%84%E5%AD%98%E5%99%A8%E6%AF%94%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E9%80%9F%E5%BA%A6%E5%BF%AB%E7%9A%84%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[同样都是晶体管存储设备，为什么寄存器比内存快呢？ Mike Ash写了一篇很好的解释，非常通俗地回答了这个问题，有助于加深对硬件的理解。 原因一：距离不同 距离不是主要因素，但是最好懂，所以放在最前面说。内存离CPU比较远，所以要耗费更长的时间读取。 以3GHz的CPU为例，电流每秒钟可以振荡30亿次，每次耗时大约为0.33纳秒。光在1纳秒的时间内，可以前进30厘米。也就是说，在CPU的一个时钟周期内，光可以前进10厘米。因此，如果内存距离CPU超过5厘米，就不可能在一个时钟周期内完成数据的读取，这还没有考虑硬件的限制和电流实际上达不到光速。相比之下，寄存器在CPU内部，当然读起来会快一点。 距离对于桌面电脑影响很大，对于手机影响就要小得多。手机CPU的时钟频率比较慢（iPhone 5s为1.3GHz），而且手机的内存紧挨着CPU。 原因二：硬件设计不同 主要是SRAM和DRAM的差异，DRAM为了节约晶体管，用了电容器维持电位，但电容器会漏电，需要周期性刷新，刷新期间不能读写。一般的内存为DRAM，寄存器为SRAM，前者比后者慢多了 苹果公司新推出的iPhone 5s，CPU是A7，寄存器有6000多位（31个64位寄存器，加上32个128位寄存器）。而iPhone 5s的内存是1GB，约为80亿位（bit）。这意味着，高性能、高成本、高耗电的设计可以用在寄存器上，反正只有6000多位，而不能用在内存上。因为每个位的成本和能耗只要增加一点点，就会被放大80亿倍。 事实上确实如此，内存的设计相对简单，每个位就是一个电容和一个晶体管，而寄存器的设计则完全不同，多出好几个电子元件。并且通电以后，寄存器的晶体管一直有电，而内存的晶体管只有用到的才有电，没用到的就没电，这样有利于省电。这些设计上的因素，决定了寄存器比内存读取速度更快。 原因三：工作方式不同 寄存器的工作方式很简单，只有两步： 找到相关的位 读取这些位 内存的工作方式就要复杂得多： 找到数据的指针。（指针可能存放在寄存器内，所以这一步就已经包括寄存器的全部工作了。） 将指针送往内存管理单元（MMU），由MMU将虚拟的内存地址翻译成实际的物理地址。 将物理地址送往内存控制器（memory controller），由内存控制器找出该地址在哪一根内存插槽（bank）上。 确定数据在哪一个内存块（chunk）上，从该块读取数据。 数据先送回内存控制器，再送回CPU，然后开始使用。 内存的工作流程比寄存器多出许多步。每一步都会产生延迟，累积起来就使得内存比寄存器慢得多。 为了缓解寄存器与内存之间的巨大速度差异，硬件设计师做出了许多努力，包括在CPU内部设置缓存、优化CPU工作方式，尽量一次性从内存读取指令所要用到的全部数据等等。 #]]></content>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java日志框架]]></title>
    <url>%2F2020%2F09%2F01%2Fjava%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[现有体系 下图是现有Java日志体系的一个示意 上图不是非常精准，但是能够比较清晰地展示现有Java日志体系的主体架构。Java日志体系大体可以分为三个部分：日志门面接口、桥接器、日志框架具体实现。 Java日志框架有很多种，最简单的是Java自带的java.util.logging，而最经典的是log4j，后来又出现了一个比log4j性能更好的logback，其他的日志框架就不怎么常用了。应用程序直接使用这些具体日志框架的API来满足日志输出需求当然是可以的，但是由于各个日志框架之间的API通常是不兼容的，这样做就使得应用程序丧失了更换日志框架的灵活性。 比直接使用具体日志框架API更合理的选择是使用日志门面接口。日志门面接口提供了一套独立于具体日志框架实现的API，应用程序通过使用这些独立的API就能够实现与具体日志框架的解耦，这跟JDBC是类似的。最早的日志门面接口是commons-logging，但目前最受欢迎的是slf4j。 日志门面接口本身通常并没有实际的日志输出能力，它底层还是需要去调用具体的日志框架API的，也就是实际上它需要跟具体的日志框架结合使用。由于具体日志框架比较多，而且互相也大都不兼容，日志门面接口要想实现与任意日志框架结合可能需要对应的桥接器，就好像JDBC与各种不同的数据库之间的结合需要对应的JDBC驱动一样。 需要注意的是，前面说过，上图并不精准，这只是主要部分，实际情况并不总是简单的“日志门面接口—&gt;桥接器—&gt;日志框架”这一条单向线。实际上，独立的桥接器有时候是不需要的，而且也并不是只有将日志门面API转调到具体日志框架API的桥接器，也存在将日志框架API转调到日志门面API的桥接器。 说白了，所谓“桥接器”，不过就是对某套API的伪实现。这种实现并不是直接去完成API所声明的功能，而是去调用有类似功能的别的API。这样就完成了从“某套API”到“别的API”的转调。如果同时存在A-to-B.jar和B-to-A.jar这两个桥接器，那么可以想象当应用程序开始调用A或者B的API时，会发生什么事。这就是最开始引出的那个stack overflow异常的基本原理。 slf4j的转接绑定 上面只是从整体上大概说了下Java现有日志体系，进一步了解一下slf4j与具体日志框架的桥接情况。 slf4j桥接到具体日志框架 下图来自slf4j官网文档Binding with a logging framework at deployment time 可以看到slf4j与具体日志框架结合的方案有很多种。当然，每种方案的最上层（绿色的应用层）都是统一的，它们向下都是直接调用slf4j提供的API（浅蓝色的抽象API层），依赖slf4j-api.jar。然后slf4j API向下再怎么做就非常自由了，几乎可以使用所有的具体日志框架。注意图中的第二层是浅蓝色的，看左下角的图例可知这代表抽象日志API，也就是说它们不是具体实现。如果像左边第一种方案那样下层没有跟任何具体日志框架实现相结合，那么日志是无法输出来的（这里不确定是否可能会默认输出到标准输出）。 图中第三层明显就不如第一、二层那么整齐划一了，因为这里已经开始涉及到了具体的日志框架。 首先看第三层中间的两个湖蓝色块，这是适配层，也就是桥接器。左边的slf4j-log4j12.jar桥接器看名字就知道是slf4j到log4j的桥接器，同样，右边的slf4j-jdk14.jar就是slf4j到Java原生日志实现的桥接器了。它们的下一层分别是对应的日志框架实现，log4j的实现代码是log4j.jar，而jul实现代码已经包含在了JVM runtime中，不需要单独的jar包。 再看第三层其余的三个深蓝色块。它们三个也是具体的日志框架实现，但是却不需要桥接器，因为它们本身就已经直接实现了slf4j API。slf4j-simple.jar和slf4j-nop.jar这两个不用多说，看名字就知道一个是slf4j的简单实现，一个是slf4j的空实现，平时用处也不大。而logback之所以也实现了slf4j API，据说是因为logback和slf4j出自同一人之手，这人同时也是log4j的作者。 第三层所有的灰色jar包都带有红框，这表示它们都直接实现了slf4j API，只是湖蓝色的桥接器对slf4j API的实现并不是直接输出日志，而是转去调用别的日志框架的API。 其它日志框架API转调回slf4j 如果只存在上面这些从sfl4j到其他日志框架的桥接器，可能还不会出什么问题。但是实际上还有另外一类桥接器，它们的作用跟上面的恰好相反，它们将其它日志框架的API转调到slf4j的API上。下图来自slf4j官网文档Bridging legacy APIs 上图展示了目前为止能安全地从别的日志框架API转调回slf4j的所有三种情形。 以左上角第一种情形为例，当slf4j底层桥接到logback框架的时候，上层允许桥接回slf4j的日志框架API有log4j和jul。jcl虽然不是什么日志框架的具体实现，但是它的API仍然是能够被转调回slf4j的。要想实现转调，方法就是图上列出的用特定的桥接器jar替换掉原有的日志框架jar。需要注意的是这里不包含logback API到slf4j API的转调，因为logback本来就是slf4j API的实现。 看完三种情形以后，会发现几乎所有其他日志框架的API，包括jcl的API，都能够随意的转调回slf4j。但是有一个唯一的限制就是转调回slf4j的日志框架不能跟slf4j当前桥接到的日志框架相同。这个限制就是为了防止A-to-B.jar跟B-to-A.jar同时出现在类路径中，从而导致A和B一直不停地互相递归调用，最后堆栈溢出。目前这个限制并不是通过技术保证的，仅仅靠开发者自己保证，这也是为什么slf4j官网上要强调所有合理的方式只有上图的三种情形。 循环依赖造成 stack overflowDetected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError.Detected both jcl-over-slf4j.jar AND slf4j-jcl.jar on the class path, preempting StackOverflowError. 原文 best practice总是使用Log Facade，而不是具体Log Implementation 使用 Log Facade 可以方便的切换具体的日志实现。而且，如果依赖多个项目，使用了不同的Log Facade，还可以方便的通过 Adapter 转接到同一个实现上。如果依赖项目使用了多个不同的日志实现，就麻烦的多了。 只添加一个 Log Implementation依赖 毫无疑问，项目中应该只使用一个具体的 Log Implementation，建议使用 Logback 或者Log4j2。如果有依赖的项目中，使用的 Log Facade不支持直接使用当前的 Log Implementation，就添加合适的桥接器依赖。 具体的日志实现依赖应该设置为optional和使用runtime scope 在项目中，Log Implementation的依赖强烈建议设置为runtime scope，并且设置为optional。例如项目中使用了 SLF4J 作为 Log Facade，然后想使用 Log4j2 作为 Implementation，那么使用 maven 添加依赖的时候这样设置: 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 设为optional，依赖不会传递，这样如果你是个lib项目，然后别的项目使用了你这个lib，不会被引入不想要的Log Implementation 依赖； Scope设置为runtime，是为了防止开发人员在项目中直接使用Log Implementation中的类，而不适用Log Facade中的类。 如果有必要, 排除依赖的第三方库中的Log Impementation依赖 第三方库的开发者未必会把具体的日志实现或者桥接器的依赖设置为optional，然后你的项目继承了这些依赖——具体的日志实现未必是你想使用的，比如他依赖了Log4j，你想使用Logback，这时就很尴尬。另外，如果不同的第三方依赖使用了不同的桥接器和Log实现，也极容易形成环。 这种情况下，推荐的处理方法，是使用exclude来排除所有的这些Log实现和桥接器的依赖，只保留第三方库里面对Log Facade的依赖。 比如阿里的JStorm就没有很好的处理这个问题，依赖jstorm会引入对Logback和log4j-over-slf4j的依赖，如果你想在自己的项目中使用Log4j或其他Log实现的话，就需要加上excludes: 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.alibaba.jstorm&lt;/groupId&gt; &lt;artifactId&gt;jstorm-core&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 避免为不会输出的log付出代价 Log库都可以灵活的设置输出界别，所以每一条程序中的log，都是有可能不会被输出的。这时候要注意不要额外的付出代价。 先看两个有问题的写法： 12logger.debug(&quot;start process request, url: &quot; + url);logger.debug(&quot;receive request: &#123;&#125;&quot;, toJson(request)); 第一条是直接做了字符串拼接，所以即使日志级别高于debug也会做一个字符串连接操作；第二条虽然用了SLF4J/Log4j2 中的懒求值方式来避免不必要的字符串拼接开销，但是toJson()这个函数却是都会被调用并且开销更大。 推荐的写法如下: 123456logger.debug(&quot;start process request, url:&#123;&#125;&quot;, url); // SLF4J/LOG4J2logger.debug(&quot;receive request: &#123;&#125;&quot;, () -&gt; toJson(request)); // LOG4J2logger.debug(() -&gt; &quot;receive request: &quot; + toJson(request)); // LOG4J2if (logger.isDebugEnabled()) &#123; // SLF4J/LOG4J2 logger.debug(&quot;receive request: &quot; + toJson(request)); &#125; 日志格式中最好不要使用行号，函数名等字段 原因是，为了获取语句所在的函数名，或者行号，log库的实现都是获取当前的stacktrace，然后分析取出这些信息，而获取stacktrace的代价是很昂贵的。如果有很多的日志输出，就会占用大量的CPU。在没有特殊需要的情况下，建议不要在日志中输出这些这些字段。 最后， log中不要输出稀奇古怪的字符！ 部分开发人员为了方便看到自己的log，会在log语句中加上醒目的前缀，比如: 1logger.debug(&quot;========================start process request=============&quot;); 虽然对于自己来说是方便了，但是如果所有人都这样来做的话，那log输出就没法看了！正确的做法是使用grep 来看只自己关心的日志。]]></content>
      <tags>
        <tag>java</tag>
        <tag>SLF4j</tag>
        <tag>log4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL慢查询日志]]></title>
    <url>%2F2019%2F10%2F23%2FMySQL%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[查看是否开启慢查询功能12345678mysql&gt; show variables like &apos;slow_query%&apos;;+---------------------+------------------------------------+| Variable_name | Value |+---------------------+------------------------------------+| slow_query_log | OFF || slow_query_log_file | /var/lib/mysql/instance-1-slow.log |+---------------------+------------------------------------+2 rows in set (0.01 sec) 1234567mysql&gt; show variables like &apos;long_query_time&apos;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+1 row in set (0.00 sec) 说明： slow_query_log 慢查询开启状态 slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） long_query_time 查询超过多少秒才记录 配置临时配置默认没有开启慢查询日志记录，通过命令临时开启： 12345678mysql&gt; set global slow_query_log=&apos;ON&apos;;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global slow_query_log_file=&apos;/var/lib/mysql/instance-1-slow.log&apos;;Query OK, 0 rows affected (0.00 sec) mysql&gt; set global long_query_time=2;Query OK, 0 rows affected (0.00 sec) 永久配置修改配置文件达到永久配置状态：12345/etc/mysql/conf.d/mysql.cnf[mysqld]slow_query_log = ONslow_query_log_file = /var/lib/mysql/instance-1-slow.loglong_query_time = 2 配置好后，重新启动 MySQL 即可。测试通过运行下面的命令，达到问题 SQL 语句的执行： 1234567mysql&gt; select sleep(2);+----------+| sleep(2) |+----------+| 0 |+----------+1 row in set (2.00 sec) 然后查看慢查询日志内容： 123456789101112$ cat /var/lib/mysql/instance-1-slow.log/usr/sbin/mysqld, Version: 8.0.13 (MySQL Community Server - GPL). started with:Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sockTime Id Command Argument/usr/sbin/mysqld, Version: 8.0.13 (MySQL Community Server - GPL). started with:Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sockTime Id Command Argument# Time: 2018-12-18T05:55:15.941477Z# User@Host: root[root] @ localhost [] Id: 53# Query_time: 2.000479 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0SET timestamp=1545112515;select sleep(2);]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查看数据库表容量大小]]></title>
    <url>%2F2019%2F10%2F23%2FMySQL%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%AE%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[1. 查看所有数据库容量大小12345678select table_schema as &apos;数据库&apos;,sum(table_rows) as &apos;记录数&apos;,sum(truncate(data_length/1024/1024, 2)) as &apos;数据容量(MB)&apos;,sum(truncate(index_length/1024/1024, 2)) as &apos;索引容量(MB)&apos;from information_schema.tablesgroup by table_schemaorder by sum(data_length) desc, sum(index_length) desc; 2. 查看所有数据库各表容量大小12345678select table_schema as &apos;数据库&apos;,table_name as &apos;表名&apos;,table_rows as &apos;记录数&apos;,truncate(data_length/1024/1024, 2) as &apos;数据容量(MB)&apos;,truncate(index_length/1024/1024, 2) as &apos;索引容量(MB)&apos;from information_schema.tablesorder by data_length desc, index_length desc; 3.查看指定数据库容量大小1234567select table_schema as &apos;数据库&apos;,sum(table_rows) as &apos;记录数&apos;,sum(truncate(data_length/1024/1024, 2)) as &apos;数据容量(MB)&apos;,sum(truncate(index_length/1024/1024, 2)) as &apos;索引容量(MB)&apos;from information_schema.tableswhere table_schema=&apos;mysql&apos;; 4.查看指定数据库各表容量大小123456789select table_schema as &apos;数据库&apos;,table_name as &apos;表名&apos;,table_rows as &apos;记录数&apos;,truncate(data_length/1024/1024, 2) as &apos;数据容量(MB)&apos;,truncate(index_length/1024/1024, 2) as &apos;索引容量(MB)&apos;from information_schema.tableswhere table_schema=&apos;mysql&apos;order by data_length desc, index_length desc;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的可靠性传输？]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BC%A0%E8%BE%93%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。 如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。 数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。 RabbitMQ 生产者弄丢了数据生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。123456789101112// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 Kafka消费端弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。]]></content>
      <tags>
        <tag>MQ</tag>
        <tag>Kafka</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息的顺序性？]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何保证消息的顺序性？我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。]]></content>
      <tags>
        <tag>MQ</tag>
        <tag>Kafka</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码字符的发展史]]></title>
    <url>%2F2019%2F09%2F03%2F%E7%BC%96%E7%A0%81%E5%AD%97%E7%AC%A6%E7%9A%84%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[原文链接 鼻祖，ascii，7位(bit)范围128 计算机发明后，为了在计算机中表示字符，人们制定了一种编码，叫ASCII码。ASCII码由一个字节中的7位(bit)表示，范围是0x00 - 0x7F 共128个字符。 随之出现扩展ascii,8位范围256 后来他们突然发现，如果需要按照表格方式打印这些字符的时候，缺少了“制表符”。于是又扩展了ASCII的定义，使用一个字节的全部8位(bit)来表示字符了，这就叫扩展ASCII码。范围是0x00 - 0xFF 共256个字符。 中国最早出现gb-2312，支持中文 中国人利用连续2个扩展ASCII码的扩展区域（0xA0以后）来表示一个汉字，该方法的标准叫GB-2312。后来，日文、韩文、阿拉伯文、台湾繁体（BIG-5）……都使用类似的方法扩展了本地字符集的定义，现在统一称为 MBCS 字符集（多字节字符集）。这个方法是有缺陷的，因为各个国家地区定义的字符集有交集，因此使用GB-2312的软件，就不能在BIG-5的环境下运行（显示乱码），反之亦然。 随之出现gbk,扩展的gb-2312,包含复杂中文和繁体 使用gb-2312一段时间后，我们很快就就发现有许多人的人名没有办法在这里打出来，中国汉字真乃博大精深。于是我们不得不继续把GB2312 没有用到的码位找出来老实不客气地用上。后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。扩展之后的编码方案被称为 GBK 标准，GBK包括了GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 进一步扩展GB18030，全中国用 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS“（Double Byte Charecter Set 双字节字符集）。 统一全球，出现unicode标准字符集 统一规定2字节=1字符 之后全世界各个国家都有自己的编码标准，导致国家与国家之间的编码转换很有问题。为了把全世界人民所有的所有的文字符号都统一进行编码，于是一个叫 ISO （国际标谁化组织）的国际组织制定了”Universal Multiple-ctet Coded Character Set”，简称 UCS, 俗称 “unicode“。UNICODE 使用2个字节表示一个字符，对于ASCII里的那些英文“半角”字符，其原编码不变，只是将其长度由原来的8位扩展为16位，高位补0.而其他文化和语言的字符则全部重新统一编码。这种大气的方案在保存英文文本时会多浪费一倍的空间。这下终于好啦，全世界任何一个地区的软件，可以不用修改地就能在另一个地区运行了。虽然我用 IE 浏览日本网站，显示出我不认识的日文文字，但至少不会是乱码了。UNICODE 的范围是 0x0000 - 0xFFFF 共6万多个字符，其中光汉字就占用了4万多个 UTF-8的出现 由于字节浪费和unicode与ascii的区别，unicode在很长一段时间内无法推广，直到互联网的出现，为解决unicode如何在网络上传输的问题，于是面向传输的众多UTF（UCS Transfer Format）标准出现了，顾名思义，UTF-8就是每次8个位传输数据，而UTF-16就是每次16个位。UTF-8就是在互联网上使用最广的一种unicode的实现方式，这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。它是一种变长的编码方式，它可以使用1~4个字节表示一个符号。注意的是unicode一个中文字符占2个字节，而UTF-8一个中文字符占3个字节。 常用字符集 字符集 位数 范围 个数 作用 ASCII 7 00-7F 128 表语英语及西欧语言 ASCII扩展 8 00-FF 256 表语英语及西欧语言 ISO-8859-1 8 00-FF 256 扩展ASCII，表示西欧、希腊语等 GB2312 16 高0xA1–0xF7低0xA1–0xFE 7445个符号，包括6763个汉字 国家简体中文字符集，兼容ASCII BIG5 16 高0x81-0xFE低0x40-0x7E、0xA1-0xFE 13053个汉字 统一繁体字编码 GBK 16 高0x81-0xFE低0x40-0xFE 21886个字符 它是GB2312的扩展，加入对繁体字的支持，兼容GB2312 GB18030 8/16/32 解决了中文、日文、朝鲜语等的编码，兼容GBK UCS 16/32 国际标准 ISO 10646 定义了通用字符集 (Universal Character Set)。它是与UNICODE同类的组织，UCS-2和UNICODE兼容 Unicode 分别是UTF-8，UTF-16和UTF-32 为世界650种语言进行统一编码，兼容ISO-8859-1]]></content>
      <tags>
        <tag>编码字符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中判断字符串真实长度（中文2个字符，英文1个字符）的方法]]></title>
    <url>%2F2019%2F09%2F03%2Fjava%E4%B8%AD%E5%88%A4%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9C%9F%E5%AE%9E%E9%95%BF%E5%BA%A6%EF%BC%88%E4%B8%AD%E6%96%872%E4%B8%AA%E5%AD%97%E7%AC%A6%EF%BC%8C%E8%8B%B1%E6%96%871%E4%B8%AA%E5%AD%97%E7%AC%A6%EF%BC%89%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java中判断字段真实长度（中文2个字符，英文1个字符）的方法1234567891011121314151617public class StringLength &#123; String s1 = &quot;a&quot;; String s2 = &quot;国1a&quot;; String s3 = &quot;aa&quot;; @Test public void testLength() &#123; assert 1 == s1.getBytes().length; assert 3 == s2.length(); assert 4 == s2.getBytes(Charset.forName(&quot;GBK&quot;)).length; assert 5 == s2.getBytes().length; assert 2 == s3.getBytes().length; assert 2 == s3.getBytes(Charset.forName(&quot;GBK&quot;)).length; assert 2 == s3.length(); &#125;&#125; 为什么用GBK字符集？其实不止是gbk编码，GB2312、GB18030也是可以的，主要利用了，字符集用2个字节表示中文，1个字节表示英文、数字。当然这不是万能的，不在字符集中的字符会不准确，例如:”嗀、兀、鎴、戝、枩、綘”,GB2312用1个字节表示 英文字母和中文汉字在不同字符集编码下的字节数12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.UnsupportedEncodingException;public class EncodingTest &#123; @Test public void test() &#123; String en = &quot;A&quot;; String ch = &quot;人&quot;; System.out.println(&quot;英文字母：&quot; + en); printByteLength(en, &quot;GB2312&quot;); printByteLength(en, &quot;GBK&quot;); printByteLength(en, &quot;GB18030&quot;); printByteLength(en, &quot;ISO-8859-1&quot;); printByteLength(en, &quot;UTF-8&quot;); printByteLength(en, &quot;UTF-16&quot;); printByteLength(en, &quot;UTF-16BE&quot;); printByteLength(en, &quot;UTF-16LE&quot;); System.out.println(); System.out.println(&quot;中文汉字：&quot; + ch); printByteLength(ch, &quot;GB2312&quot;); printByteLength(ch, &quot;GBK&quot;); printByteLength(ch, &quot;GB18030&quot;); printByteLength(ch, &quot;ISO-8859-1&quot;); printByteLength(ch, &quot;UTF-8&quot;); printByteLength(ch, &quot;UTF-16&quot;); printByteLength(ch, &quot;UTF-16BE&quot;); printByteLength(ch, &quot;UTF-16LE&quot;); &#125; /** * 打印不同字符集下Java字符串所占的字节数 * * @param str 待操作的字符串 * @param encodingName 字符集名称 */ public void printByteLength(String str, String encodingName) &#123; System.out.print(&quot;字节数 : &quot;); try &#123; System.out.print(str.getBytes(encodingName).length); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;;编码：&quot; + encodingName); &#125;&#125; 结果：12345678910111213141516171819英文字母：A字节数 : 1;编码：GB2312字节数 : 1;编码：GBK字节数 : 1;编码：GB18030字节数 : 1;编码：ISO-8859-1字节数 : 1;编码：UTF-8字节数 : 4;编码：UTF-16字节数 : 2;编码：UTF-16BE字节数 : 2;编码：UTF-16LE中文汉字：人字节数 : 2;编码：GB2312字节数 : 2;编码：GBK字节数 : 2;编码：GB18030字节数 : 1;编码：ISO-8859-1字节数 : 3;编码：UTF-8字节数 : 4;编码：UTF-16字节数 : 2;编码：UTF-16BE字节数 : 2;编码：UTF-16LE]]></content>
      <tags>
        <tag>java</tag>
        <tag>String</tag>
        <tag>编码字符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP Finite State Machine]]></title>
    <url>%2F2019%2F08%2F25%2FTCP-Finite-State-Machine%2F</url>
    <content type="text"><![CDATA[TCP Operational Overview and the TCP Finite State Machine (FSM) TCP 的那些事儿(上) TCP Finite State Machine (FSM) States, Events and Transitions Table 151: TCP Finite State Machine (FSM) States, Events and Transitions State State Description Event and Transition CLOSED This is the default state that each connection starts in before the process of establishing it begins. The state is called “fictional” in the standard. The reason is that this state represents the situation where there is no connection between devices—it either hasn't been created yet, or has just been destroyed. If that makes sense. J Passive Open: A server begins the process of connection setup by doing a passive open on a TCP port. At the same time, it sets up the data structure (transmission control block or TCB) needed to manage the connection. It then transitions to the LISTEN state. Active Open, Send SYN: A client begins connection setup by sending a SYN message, and also sets up a TCB for this connection. It then transitions to the SYN-SENT state. LISTEN A device (normally a server) is waiting to receive a synchronize (SYN) message from a client. It has not yet sent its own SYN message. Receive Client SYN, Send SYN+ACK: The server device receives a SYN from a client. It sends back a message that contains its own SYN and also acknowledges the one it received. The server moves to the SYN-RECEIVED state. SYN-SENT The device (normally a client) has sent a synchronize (SYN) message and is waiting for a matching SYN from the other device (usually a server). Receive SYN, Send ACK: If the device that has sent its SYN message receives a SYN from the other device but not an ACK for its own SYN, it acknowledges the SYN it receives and then transitions to SYN-RECEIVED to wait for the acknowledgment to its SYN. Receive SYN+ACK, Send ACK: If the device that sent the SYN receives both an acknowledgment to its SYN and also a SYN from the other device, it acknowledges the SYN received and then moves straight to the ESTABLISHED state. SYN-RECEIVED The device has both received a SYN (connection request) from its partner and sent its own SYN. It is now waiting for an ACK to its SYN to finish connection setup. Receive ACK: When the device receives the ACK to the SYN it sent, it transitions to the ESTABLISHED state. ESTABLISHED The “steady state” of an open TCP connection. Data can be exchanged freely once both devices in the connection enter this state. This will continue until the connection is closed for one reason or another. Close, Send FIN: A device can close the connection by sending a message with the FIN (finish) bit sent and transition to the FIN-WAIT-1 state. Receive FIN: A device may receive a FIN message from its connection partner asking that the connection be closed. It will acknowledge this message and transition to the CLOSE-WAIT state. CLOSE-WAIT The device has received a close request (FIN) from the other device. It must now wait for the application on the local device to acknowledge this request and generate a matching request. Close, Send FIN: The application using TCP, having been informed the other process wants to shut down, sends a close request to the TCP layer on the machine upon which it is running. TCP then sends a FIN to the remote device that already asked to terminate the connection. This device now transitions to LAST-ACK. LAST-ACK A device that has already received a close request and acknowledged it, has sent its own FIN and is waiting for an ACK to this request. Receive ACK for FIN: The device receives an acknowledgment for its close request. We have now sent our FIN and had it acknowledged, and received the other device's FIN and acknowledged it, so we go straight to the CLOSED state. FIN-WAIT-1 A device in this state is waiting for an ACK for a FIN it has sent, or is waiting for a connection termination request from the other device. Receive ACK for FIN: The device receives an acknowledgment for its close request. It transitions to the FIN-WAIT-2 state. Receive FIN, Send ACK: The device does not receive an ACK for its own FIN, but receives a FIN from the other device. It acknowledges it, and moves to the CLOSING state. FIN-WAIT-2 A device in this state has received an ACK for its request to terminate the connection and is now waiting for a matching FIN from the other device. Receive FIN, Send ACK: The device receives a FIN from the other device. It acknowledges it and moves to the TIME-WAIT state. CLOSING The device has received a FIN from the other device and sent an ACK for it, but not yet received an ACK for its own FIN message. Receive ACK for FIN: The device receives an acknowledgment for its close request. It transitions to the TIME-WAIT state. TIME-WAIT The device has now received a FIN from the other device and acknowledged it, and sent its own FIN and received an ACK for it. We are done, except for waiting to ensure the ACK is received and prevent potential overlap with new connections. (See the topic describing connection termination for more details on this state.) Timer Expiration: After a designated wait period, device transitions to the CLOSED state. tcp 三次握手 四次挥手 对于建链接的3次握手，主要是要初始化Sequence Number 的初始值。通信的双方要互相通知对方自己的初始化的Sequence Number（缩写为ISN：Inital Sequence Number）——所以叫SYN，全称Synchronize Sequence Numbers。也就上图中的 x 和 y。这个号要作为以后的数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输的问题而乱序（TCP会用这个序号来拼接数据）。 对于4次挥手，其实你仔细看是2次，因为TCP是全双工的，所以，发送方和接收方都需要Fin和Ack。只不过，有一方是被动的，所以看上去就成了所谓的4次挥手。如果两边同时断连接，那就会就进入到CLOSING状态，然后到达TIME_WAIT状态。下图是双方同时断连接的示意图（你同样可以对照着TCP状态机看）:]]></content>
      <tags>
        <tag>tcp</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Registry Mirror]]></title>
    <url>%2F2019%2F08%2F24%2FDocker-Registry-Mirror%2F</url>
    <content type="text"><![CDATA[docker有官方的中国区 https://registry.docker-cn.com 网易163 docker镜像 http://hub-mirror.c.163.com/ ustc的镜像Docker Hub 源使用帮助 https://docker.mirrors.ustc.edu.cn daocloudDaoCloud也提供了docker加速器，需要用户注册后才能使用，并且每月限制流量10GB。 https://www.daocloud.io/mirror#accelerator-doc http://xxxx.m.daocloud.io aliyun阿里云也提供了docker加速器，注册为阿里云的用户，还得加入开发者平台 https://yourcode.mirror.aliyuncs.com azure http://dockerhub.azk8s.cn tencent https://mirror.ccs.tencentyun.com]]></content>
      <tags>
        <tag>docker</tag>
        <tag>mirror</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次频繁Full GC问题排查]]></title>
    <url>%2F2019%2F07%2F21%2F%E4%B8%80%E6%AC%A1%E9%A2%91%E7%B9%81Full-GC%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[频繁Full GC问题排查定位先上结论：长时间运行MetricsClientHttpRequestInterceptor.servoMonitorCache.timerCache积累过多且不回收，导致频繁fullGc 起因：运维的同事说，你这个服务运行时间长了，内存不回收啊，内存一直往上加，我们半月就得重启一次，要不就返回503了，我心想不可能啊，java自带内存回收，后来一看系统监控果然，内存大趋势随着时间线性增长，很稳定，放大之后发现，内存是有回收的，只是每次的最低点都在提高，我第一反应是，对象到老年代了，一直在养老，钉子户越来越多导致的，那就铲除钉子户吧。 jstat -gcutil [pid] 1000 查看gc执行频率，确实一直在Full GC ，而且还收不干净。 jmap -F -dump:format=b,file=heapDump [pid] 先保留现场，不能让服务一直不可用，留好快照，让运维的同事先重启解决下。 用eclipse mat 工具打开dump文件，文件太大，3.13g,需要修改mat启动参数,如图: 主要是这两行-Xmx4096m,-Xms1024m 点击Leak Suspects，查找内存泄漏 发现内存泄漏的class, org.springframework.cloud.netflix.metrics.MetricsClientHttpRequestInterceptor org.springframework.cloud.netflix.metrics.servo.ServoMonitorCache(servoMonitorCache) java.util.HashMap(timerCache) 回到源码，查找对应class,发现 Intercepts RestTemplate requests and records metrics about execution time and results. 原来是为了统计执行时间和返回结果的，那就关掉吧。 在MetricsInterceptorConfiguration中 1@ConditionalOnProperty(value = &quot;spring.cloud.netflix.metrics.enabled&quot;, havingValue = &quot;true&quot;, matchIfMissing = true) 默认是true，需要在自己的配置文件中添加spring.cloud.netflix.metrics.enabled=false 发版本，上线，观察内存监控，很平稳。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>FullGC</tag>
        <tag>jmap</tag>
        <tag>jstat</tag>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器cpu100%怎么定位？]]></title>
    <url>%2F2019%2F07%2F21%2F%E6%9C%8D%E5%8A%A1%E5%99%A8cpu100-%E6%80%8E%E4%B9%88%E5%AE%9A%E4%BD%8D%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？ 找到最耗CPU的进程 执行top -c ，显示进程运行信息列表 键入P (大写p)，进程按照CPU使用率排序 如上图，最耗CPU的进程PID为29025 找到最耗CPU的线程 top -Hp 29025 ，显示一个进程的线程运行信息列表 键入P (大写p)，线程按照CPU使用率排序 如图 进程29025 最耗cpu的线程pid是29327，29119 将线程PID转化为16进制 printf “%x,%x” 29327 29119 输出 “728f,71bf” 之所以要转化为16进制，是因为堆栈里，线程id是用16进制表示的。 查看堆栈，找到线程在干嘛 jstack 29025 | grep -e “728f” -e “71bf” -C5 —color 如上图找到cpu高的线程对应的线程名字为 “netty-worker-2-1”，”netty-worker-2-2”，以及看到了该线程正在执行代码的堆栈。]]></content>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 单例模式中双重检查锁定 volatile 的作用]]></title>
    <url>%2F2019%2F07%2F16%2Fjava-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B8%AD%E5%8F%8C%E9%87%8D%E6%A3%80%E6%9F%A5%E9%94%81%E5%AE%9A-volatile-%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[volatile 是保证了可见性还是有序性？有序性：是因为 instance = new Singleton(); 不是原子操作。编译器存在指令重排，从而存在线程1 创建实例后（初始化未完成），线程2 判断对象不为空，但实际对象扔为空，造成错误。 可见性：是因为线程1 创建实例后还只存在自己线程的工作内存，未更新到主存。线程 2 判断对象为空，创建实例，从而存在多实例错误。 结论主要是禁止重排序，初始化一个实例（SomeType st = new SomeType()）在java字节码中会有4个步骤， 申请内存空间 初始化默认值（区别于构造器方法的初始化） 执行构造器方法 连接引用和实例 这4个步骤后两个有可能会重排序，1234 1243都有可能，造成未初始化完全的对象发布。volatile可以禁止指令重排序，从而避免这个问题。 为什么要禁止重排序？确保先执行构造器方法，再将引用和实例连接到一起。如果没有禁止重排序，会导致另一个线程可能获取到尚未构造完成的对象。 为什么没有起到可见性的作用？ JSR-133 An unlock on a monitor happens before every subsequent lock on that same monitor 第二次非null判断是在加锁以后，则根据这一条，另一个线程一定能看到这个引用被赋值。所以即使没有volatile，依旧能保证可见性。 如果不加volatile，能不能使代码正确运行？既然可见性已经有了保证，那我们只需要保证有序性。怎么保证有序性呢？ 只需要在“构造对象”和“连接引用与实例”之间加上一道内存屏障 由于是在单线程里，同样根据JSR-133 Each action in a thread happens before every action in that thread that comes later in the program’s order DoubleCheckedLocking]]></content>
      <tags>
        <tag>java</tag>
        <tag>单例模式</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F07%2F09%2FHashMap%2F</url>
    <content type="text"><![CDATA[HashMap数据结构JDK1.8以前HashMap的实现是 数组+链表JDK1.8开始HashMap的实现是 数组+链表/红黑树，如下图： HashMap类中有两个常量：12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; 当链表中节点数量大于等于TREEIFY_THRESHOLD时，链表会转成红黑树。 当链表中节点数量小于等于UNTREEIFY_THRESHOLD时，红黑树会转成链表。 为什么TREEIFY_THRESHOLD的默认值被设定为8？HashMap中有这样一段注释:12345678910111213141516171819202122232425/* Because TreeNodes are about twice the size of regular nodes, we * use them only when bins contain enough nodes to warrant use * (see TREEIFY_THRESHOLD). And when they become too small (due to * removal or resizing) they are converted back to plain bins. In * usages with well-distributed user hashCodes, tree bins are * rarely used. Ideally, under random hashCodes, the frequency of * nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average for the default resizing * threshold of 0.75, although with a large variance because of * resizing granularity. Ignoring variance, the expected * occurrences of list size k are (exp(-0.5) * pow(0.5, k) / * factorial(k)). The first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million */ 当hashCode离散性很好的时候，树型bin用到的概率非常小，因为数据均匀分布在每个bin中，几乎不会有bin中链表长度会达到阈值。但是在随机hashCode下，离散性可能会变差，然而JDK又不能阻止用户实现这种不好的hash算法，因此就可能导致不均匀的数据分布。不过理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，我们可以看到，一个bin中链表长度达到8个元素的概率为0.00000006，几乎是不可能事件。所以，之所以选择8，不是拍拍屁股决定的，而是根据概率统计决定的。]]></content>
      <tags>
        <tag>java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effects of Type Erasure and Bridge Methods]]></title>
    <url>%2F2019%2F07%2F04%2FEffects-of-Type-Erasure-and-Bridge-Methods%2F</url>
    <content type="text"><![CDATA[原文链接 Effects of Type ErasureSometimes type erasure causes a situation that you may not have anticipated. The following example shows how this can occur. The example (described in Bridge Methods) shows how a compiler sometimes creates a synthetic method, called a bridge method, as part of the type erasure process. Given the following two classes: 1234567891011121314151617181920public class Node&lt;T&gt; &#123; public T data; public Node(T data) &#123; this.data = data; &#125; public void setData(T data) &#123; System.out.println(&quot;Node.setData&quot;); this.data = data; &#125;&#125;public class MyNode extends Node&lt;Integer&gt; &#123; public MyNode(Integer data) &#123; super(data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125;&#125; Consider the following code:1234MyNode mn = new MyNode(5);Node n = mn; // A raw type - compiler throws an unchecked warningn.setData(&quot;Hello&quot;); Integer x = mn.data; // Causes a ClassCastException to be thrown. After type erasure, this code becomes:1234MyNode mn = new MyNode(5);Node n = (MyNode)mn; // A raw type - compiler throws an unchecked warningn.setData(&quot;Hello&quot;);Integer x = (String)mn.data; // Causes a ClassCastException to be thrown. Here is what happens as the code is executed: n.setData(“Hello”); causes the method setData(Object) to be executed on the object of class MyNode. (The MyNode class inherited setData(Object) from Node.) In the body of setData(Object), the data field of the object referenced by n is assigned to a String. The data field of that same object, referenced via mn, can be accessed and is expected to be an integer (since mn is a MyNode which is a Node. Trying to assign a String to an Integer causes a ClassCastException from a cast inserted at the assignment by a Java compiler. Bridge MethodsWhen compiling a class or interface that extends a parameterized class or implements a parameterized interface, the compiler may need to create a synthetic method, called a bridge method, as part of the type erasure process. You normally don’t need to worry about bridge methods, but you might be puzzled if one appears in a stack trace. After type erasure, the Node and MyNode classes become: 123456789101112131415161718192021public class Node &#123; public Object data; public Node(Object data) &#123; this.data = data; &#125; public void setData(Object data) &#123; System.out.println(&quot;Node.setData&quot;); this.data = data; &#125;&#125;public class MyNode extends Node &#123; public MyNode(Integer data) &#123; super(data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125;&#125; After type erasure, the method signatures do not match. The Node method becomes setData(Object) and the MyNode method becomes setData(Integer). Therefore, the MyNode setData method does not override the Node setData method. To solve this problem and preserve the polymorphism of generic types after type erasure, a Java compiler generates a bridge method to ensure that subtyping works as expected. For the MyNode class, the compiler generates the following bridge method for setData: 123456789101112131415class MyNode extends Node &#123; // Bridge method generated by the compiler // public void setData(Object data) &#123; setData((Integer) data); &#125; public void setData(Integer data) &#123; System.out.println(&quot;MyNode.setData&quot;); super.setData(data); &#125; // ...&#125; As you can see, the bridge method, which has the same method signature as the Node class’s setData method after type erasure, delegates to the original setData method.]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm-方法调用]]></title>
    <url>%2F2019%2F07%2F04%2Fjvm-%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[重载与重写重载在java程序里，如果同一个类中出现多个名字相同，并且参数类型相同的方法，那么它无法通过编译。也就是说，在正常情况下，如果我们想要在同一个类中定义名字相同的方法，那么它们的参数类型必须不同。这些方法之间的关系，我们称之为重载。 重载的方法在编译过程中即可完成识别。具体到每一个方法调用，java编译器会根据所传入参数的声明类型（注意与实际类型区分）来选取重载方法。选取的过程共分为三个阶段： 在不考虑对基本类型自动拆装箱（auto-boxing，auto-unboxing），以及可变长参数的情况下选取重载方法； 如果在第1个阶段中没有找到适配的方法，那么在允许自动拆装箱，但不允许可变长参数的情况下选取重载方法； 如果在第2阶段中没有找到适配的方法，那么在允许自动拆装箱以及可变长的情况下选取重载方法。 如果java编译器在同一阶段中找到了多个适配的方法，那么它会在其中选择一个最为贴切的，而决定贴切程度的一个关键就是形式参数类型的继承关系。例如： void invoke(Object,Object... args){...} void invoke(String s,Object obj,Object... args){...}第一个参数当传入null时，由于String是Object的子类，因此java编译器会认为第二个方法更为贴切。 重写如果子类定义了与父类中非私有方法同名的方法，而且这个方法的参数类型相同，并且这两个方法都不是静态的，那么子类的方法重写了父类中的方法。 jvm的静态绑定和动态绑定java虚拟机识别方法的关键在于类名、方法名以及方法描述符（method descriptor）。方法描述符是由方法的参数类型以及返回类型所构成。如果同一个类中同时出现多个名字相同且描述符也相同的方法，那么java虚拟机会在类的验证阶段报错。 对于java语言中重写而java虚拟机中非重写的情况，编译器会通过类型擦除和方法桥接来实现java中的重写语义。 由于对重载方法的区分在编译阶段已经完成，所以java虚拟机不存在重载概念。 在java虚拟机中，静态绑定指的是在解析时便能够直接识别目标方法的情况，而动态绑定指的是需要在运行过程中根据调用者的动态类型来识别目标方法的情况。 java字节码中与调用相关的指令有5种： invokestatic：用于调用静态方法。 invokespecial：用于调用私有实例方法、构造器，以及使用super关键字调用父类的实例方法或构造器，和所有接口的默认方法。 invokevirtual：用于调用非私有实例方法。 invokeinterface：用于调用接口方法。 invokedynamic：用于调用动态方法。 虚方法调用对于invokevirtual和invokeinterface而言，在绝大部分年情况下，虚拟机需要在执行过程中，根据调用者的动态类型，来确定具体的目标方法。这两种指令均属于java虚拟机中的虚方法调用，这个过程也称为动态绑定。相对于静态绑定的非虚方法调用来说，虚方法调用更加耗时。唯一的例外在于，如果虚拟机能够确定目标方法有且仅有一个，比如虚方法调用目标方法被标记为final方法，那么也可以不通过动态绑定，使用静态绑定该虚调用的目标方法。 对于invokestatic和invokespecial而言，java虚拟机能够直接识别具体的目标方法。这个过程属于静态绑定 方法表java虚拟机中采取了一种空间换取时间的策略来实现动态绑定，在类加载的准备阶段，它除了为静态字段分配内存，还会构造与该类相关联的方法表，用以快速定位目标方法。 invokevirtual使用虚方法表（virtual method table，vtable），invokeinterface使用接口方法表（interface method table，itable），接口方法表稍微复杂，但原理类似。 方法表本质是一个数组每个数组元素指向一个当前类及其祖先类中非私有实例方法。 子类方法表包含父类方法表的所有方法； 子类方法表在方法表中的索引值与它所重写的父类方法的索引值相同。 实际上，使用了方法表的动态绑定与静态绑定相比，仅仅多出几个内存解引用操作。访问栈上的调用者，读取调用者的动态类型，读取该类型的方法表，读取方法表中某个索引值所对应的目标方法。相对于创建并初始化java栈帧来说，这几个内存解引用操作的开销简直可以忽略不计。 内联缓存方法表优化实际仅存在于解释执行中，或者即时编译代码的最坏情况。即时编译还拥有另外两种性能更好的优化手段：内联缓存（inlining cache）和方法内联（method inlining）。 内联缓存是一种加快动态绑定的优化技术。它能够缓存虚方法调用中调用者的动态类型，以及该类型所对应的目标方法。在之后的执行过程中，如果碰到已缓存的类型，内联缓存便会直接调用该类型所对应的目标方法。如果没有碰到已缓存的类型，内联缓存则会退化至使用基于方法表的动态绑定。 单态（monomorphic）指的是仅有一种状态的情况。 多态（polymorphic）指的是有限数量种状态的情况。二态（bimorphic）是多态的其中一种。 超多态（megamorphic）指的是更多种状态的情况。通常用一个具体数值来区分多态和超多态。 当内联缓存没有命中的情况下，java虚拟机需要重新使用方法表进行动态绑定。对于内联缓存中的内容，java虚拟机选择劣化为超多态，处于这种状态下的内联缓存，实际上放弃了优化的机会。直接访问方法表，来动态绑定目标方法。 单态内联缓存和超多态内联缓存的性能差距 Conditions for inlining methods by the HotSpot VM It doesn’t matter how many sub-class a class has, the only thing which matter is how many methods could be called from a given line of code. e.g. a method could have two implementations across four class, but if only one is called, it will be as if the methods had only one implementation. Performance of inlined virtual method invocations in Java]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm-类加载机制]]></title>
    <url>%2F2019%2F07%2F01%2Fjvm-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类加载过程 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括了：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（using）、和卸载（Unloading）七个阶段。其中验证、准备和解析三个部分统称为连接（Linking），如下如所示。 加载，是指查找字节流，并据此创建类的过程。 链接 验证，确保被加载的类能够满足java虚拟机的约束条件。 准备，为被加载类的静态字段分配内存。 解析，将符号引用解析为实际引用。如果符号引用指向一个未被加载的类或未被加载类的字段或方法，那么解析将触发这个类的加载（但未必出发这个类的链接和初始化。） 初始化，为标记为常量值的字段赋值，以及执行clinit方法。 clinit（如果直接赋值的静态字段被final所修饰，并且它的类型是基本类型或字符串时，那么该字段便会被java编译器标记成常量，其初始化直接有java虚拟机完成。除此之外的直接赋值操作，以及所有静态代码块中的代码，则会被java编译器置于同一方法中，并把它命名为clinit） 类的初始化何时会被出发，JVM规范枚举了下述情况： 当虚拟机启动时，初始化用户指定的主类。 当遇到用以新建目标实例的new指令时，初始化new指令的目标类。 当遇到调用静态方法的指令时，初始化该静态方法所在的类。 当遇到访问静态字段的指令时，初始化该静态字段所在的类。 子类初始化会触发父类的初始化。 如果接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，会触发该接口的初始化。 使用反射api对某个类进行反射调用时，初始化这个类。 当初次调用methodHandle实例时，初始化该MethodHandle指向的方法所在的类。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Happens-Before规则]]></title>
    <url>%2F2019%2F06%2F17%2FHappens-Before%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[Happens-Before的7个规则 程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而”后面”是指时间上的先后顺序。 volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的”后面”同样是指时间上的先后顺序。 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join（）方法结束、Thread.isAlive（）的返回值等手段检测到线程已经终止执行。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生。 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。]]></content>
      <tags>
        <tag>java</tag>
        <tag>并发编程</tag>
        <tag>java内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程的生命周期]]></title>
    <url>%2F2019%2F06%2F14%2Fjava%E7%BA%BF%E7%A8%8B%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[通用的线程生命周期 通用的线程生命周期基本上可以用下图这个“五态模型”来描述。这五态分别是：初始状态、可运行状态、运行状态、休眠状态、终止状态 1.初始状态 线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。 2.可运行状态 线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。 3.运行状态 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了运行状态。 4.休眠状态 运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。 5.终止状态 线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。 这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 除了简化合并，这五种状态也有可能被细化，比如，Java 语言里就细化了休眠状态（这个下面我们会详细讲解）。 Java 中线程的生命周期 Java 语言中线程共有六种状态，分别是： NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） RUNNABLE 与 BLOCKED 的状态转换 只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。 如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。 而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。 RUNNABLE 与 WAITING 的状态转换 获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。其中，wait() 方法我们在上一篇讲解管程的时候已经深入介绍过了，这里就不再赘述。 调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。 调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。 RUNNABLE 与 TIMED_WAITING 的状态转换 调用带超时参数的 Thread.sleep(long millis) 方法； 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法； 调用带超时参数的 Thread.join(long millis) 方法； 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法； 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。 从 NEW 到 RUNNABLE 状态 Java 刚创建出来的 Thread 对象就是 NEW 状态，而创建 Thread 对象主要有两种方法。 继承 Thread 对象，重写 run() 方法。 123456789// 自定义线程对象class MyThread extends Thread &#123; public void run() &#123; // 线程需要执行的代码 ...... &#125;&#125;// 创建线程对象MyThread myThread = new MyThread(); 实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数。 12345678910// 实现 Runnable 接口class Runner implements Runnable &#123; @Override public void run() &#123; // 线程需要执行的代码 ...... &#125;&#125;// 创建线程对象Thread thread = new Thread(new Runner()); 从 RUNNABLE 到 TERMINATED 状态 线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。 那 stop() 和 interrupt() 方法的主要区别是什么呢？ stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了，所以这里也就不多介绍了。 而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。 上面这两种情况属于被中断的线程通过异常的方式获得了通知。还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了。 Java线程状态切换]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于长连接的负载均衡解决方案]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%85%B3%E4%BA%8E%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[最近公司培训，讲到了有关于长连接的负载均衡解决方案，确有体会，故此记录一下。在之前做负载均衡一般针对的是短连接，短连接的场景在实际应用中非常普遍。浏览器中大部分的请求都是短连接，例如用户登录，注册。商城的订单，付款等功能都属于短连接。短连接的特点就是无状态，连接时间短，长则三四秒，短则几毫秒。短连接的负载均衡很容易解决，开源中间件也比较多,例如nginx，F5等。而长连接的负载均衡解决方案则比较少了，主要原因是长连接相对于短连接来说应用面比较窄。一般是定制化需求会使用长连接。而生活中长连接作为普遍的应用就是直播系统了，近几年直播这种娱乐方式也越来越受到年轻人的喜爱，虎牙，斗鱼等直播平台如雨后春笋般涌现。我所在的公司主要从事安防行业，其中最为普遍的业务是摄像头的录像，在城市中每个街头，小区，公交地铁上的摄像头都会接入到公安体系中，其中录像不但能实时播放而且还会保存到服务器中，方便公安人员破案时可以随时查看录像。由于摄像机是24小时录像的，所以在这种场景下摄像机是使用长连接，而且一旦摄像机和某个服务器节点建立连接，就会长期和这个服务器保持着连接。一个城市的摄像头成千上万，后台不但需要考虑并发，还要考虑负载均衡。传统的负载均衡算法如轮询，哈希，随机等算法并不适用于长连接的一些业务场景。这里长连接可比作一个持续进行的任务，那么长连接的负载均衡就是每个任务的资源调度，最终使每个节点上的资源，均匀的分布在这些任务上。 长连接的特点由于业务场景不同，长连接的特点也不一样。不同的业务场景会有各种各样的特点，需要做定制化需求的开发，这里例举的摄像机长连接特点： 每个长连接任务的时间长，短则几天，长则几个月。并且长期占用服务器的资源。 每个长连接任务消耗的资源不一样。以摄像机为例，不同规格的摄像机像素不一样。有标清，高清，超清之分。对应的就是流量带宽的区别，有2M，4M，8M之分。因此每个长连接任务占用的资源大小不一样的。 长连接任务启动时，初始化较慢，例如需要做认证，或是请求一些接口来获取数据。这样会导致做负载均衡时，并不能实时获取到节点的负载情况，出现几秒钟的延时。 在这些特点下如何做负载均衡呢？短连接的负载均衡算法轮询，哈希，随机，平均负载。如果这些算法能不能直接应用于长连接的业务场景，会出现哪些问题呢？我们先从最简单的特点开始，逐渐递进的去思考这个问题，从无到有的推导出一个针对这些特点的负载均衡算法。上面三个特点以下简称特点一，特点二，特点三。 满足特点一，不考虑特点二，三这种情景下长连接任务只是时间长，但每个长连接任务消耗资源大小一样，启动时无延时。这种场景是最为简单也是最理想的场景，在这种场景下轮询，哈希，随机算法都适用。因为每个任务大小一样，当任务数量够多时就能保证任务是平均分配到节点上的，从而保证了每个节点的负载基本一致，这些算法都能使服务器节点达到较为均匀的负载状态，这种情况根短连接的情况差不多，只是任务时间长一些。如下图 满足特点一，二，不考虑特点三这种场景下任务启动时没有延时。但由于每个任务大小不一样轮询，哈希，随机就也就不再适应了。因为这几种算法适用于请求数量很大，任务大小差别不大的情况。也就是样本数足够大，样本基本一致才能保证这些样本均匀的分布在节点上。但也仅仅只是保证了每个节点上的样本数量均匀分布，倘若每个样本的所占用的资源不一样。那么这种均匀分配实际并不会使服务器的节点能够均匀的负载，达不到负载均衡的效果。那么这样的场景下，只平均负载的算法能够适用与这种场景。平均负载算法是在每次派发任务时，会收到节点反馈过来的负载情况，然后根据每个节点的负载情况，选择负载最小的节点派发任务。下面以图来说明这两种情况，如下图从图上可知，有任务a,b,c分别对应三个摄像机。任务a是2M流量，任务b是4M流量，任务c是8M流量，假设每个节点配置都一样，满载时为100。很明显如果使用轮询节点c的负载肯定会大，这样实际上集群负载并不均衡。但如果使用平均负载，那么负载均衡器在派发任务时会根据节点的负载情况去分发任务，这样最终效果肯定是要比轮询好的 满足特点一，二，三其实在前面两个特点下，我们忽略了一个重要问题。就是可信度问题，也就是特点三带来的问题。在分布式环境中，节点之间通信是不可靠。如果由于网络波动，或是业务场景导致负载均衡器在得到节点反馈过来的负载信息时，这个负载信息可能是存在几秒钟的延时。实际上得到的并不是当前的实时负载情况，而是几秒钟之前的负载情况。如下图：这样会存在一个极大的隐患。假设有1000个请求过来了。根据平均负载的算法得到节点a的负载最低。那么请求理应分发到节点a上，但是由于反馈过来的负载信息是存在延时的，于是在两三秒内得到的结果都是节点a负载最低，那么这1000个请求可能会全部被分发到节点a上。这种现象被称为扎堆效应。这种情况的结果就是节点a扛不住压力宕机。那么由此可见平均负载算法也不适用了。 平均负载算法的改进(一)从上面几种场景可以看出，平均负载算法还是适用性是非常强的，而在第三种场景下如果能得到一个可信度较高的节点当前负载情况，那么就可以继续沿用平均负载算法。那么该如何得到可信的节点负载呢，由前面的分析可知得到的节点负载之所以不可信是因为存在延时，那么我们可以记录节点前三秒，或者是前五秒的每秒节点负载，在把这些节点负载取平均值作为节点的当前负载。公式如下 m=(m4+m3+m2+m1+m0)/5m表示当前负载 ，m4,m3,m2,m1,m0分别表示4,3,2,1,0秒前的负载，对于存在延时的系统，这样做法就能在一定程度上得到较为可靠节点的负载情况。但仍然不够准确，在实际生产环境中仍然会出现不均衡的情况，究其原因是因为对前5秒的数据取平均，但实际上，这样计算是不合理的，难道5秒以前的数据和1秒以前的数据对当前结果的影响还是一样的吗？显然不是，应该认为越靠近当前时间的负载值可信度就越高，越远离当前时间的负载值可信度就越低。这样才会越接近于实际。所以上面的公式应该是这样的 m= 0.36*m0+0.28*m1+0.20*m2+0.12*m3+0.04*m4其中m0离当前时间最近，认为m0的可信度最高，所以给了36%的权重，而m4离当前时间最远，可信度最低，所以给了4%的权重。当然这里的权重应根据实际情况或经验设置值，根据这个公式算出的当前节点的负载值更能反应出节点的真实负载情况 平均负载算法的改进(二)上面两个公式是在得到几秒内的负载值然后计算出加权平均值确能在一定程度上反应节点的负载情况，但是这个种方法需要多次记录节点的负载信息，还比较麻烦。那有没有更好的方法呢，答案是有的，其实只需记录上一秒的节点负载情况，然后再获取当前节点负载，在这段时间段内对节点内，节点负载对时间取积分即可。这个方法是根据自动控制原理中的PID算法得到的那么上面节点负载的计算公式可得如下 m=m0+\frac{\int_0^t{(m0-m1)*t}\,{\rm d}t}{D}公式中m表示计算得到的负载值，m0表示当前获取到的节点负载，m1表示上一秒获取到的节点负载，参数D是可信度系数，表示上一秒的节点负载值对当前负载值的影响程度，越大表示影响越小，这个值可根据实际情况设置。这样既只需要维护上一秒的节点负载值，减小了成本，又能得到相对准确的当前节点负载值。 负载均衡健康状态负载均衡的健康状态如何定义呢？如果任务均匀的分布在各个节点上，就称集群的负载均衡状态是健康的，否则就是亚健康或者说是不健康的。在前面，我们衡量集群各个节点的负载情况时，总是以节点已经负载的量去评定的，例如集群有3个节点，每个节点的满载量为100，当前每个节点放负载为40，那么就可以认为集群达到了比较均匀的负载均衡状态。但这是理想状态，实际生活中，集群的节点配置是不可能完全相同的，有些节点可能是后加入的那配置可能会高一些，有些节点是之前用了好几年的，为了节省成本继续利用起来配置自然会点一些，所以集群各个节点是完全不一样的。如下图所示a,b,c三个节点，a节点放满载量为60，b节点的满载量为140，c节点的满载量为100，这样如继续以节点已负载的量去衡量的话，假设当前每个节点放负载仍为40，就会得到如下的情况 节点 节点a 节点b 节点c 负载状态 40/60 40/140 40/100 这样明显可以看出节点b还有很大的富余，因此通过节点当前的负载量，去判断集群的负载均衡状态是不科学的，这里可以换一种思路，以每个节点的剩余量作为衡量标准，这样上面那个例子中，节点a的富余是20，节点b的富余是100，节点c的富余是60，这样得到的就是如下的情况 节点 节点a 节点b 节点c 富余量 20 100 60 在负载均衡服务器派发任务时，不再是以节点负载最小的节点作为派发对象，而是以节点富余最大的节点作为派发对象，这样就能使节点的负载尽可能的均衡。这里衍生出一个公式可计算集群负载均衡的状态，首先计算出集群各个节点的负载富余的平均值 n=\frac{(n0+n1+n2)}3然后求方差 y=(n0-n)^2+(n1-n)^2+(n2-n)^2这样计算得到的方差结果，如果结果越小则说明集群负载越均衡，状态越健康，越大则说明越不均衡，集群的负载均衡处于一个相对不健康的状态]]></content>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg 接收rtsp时的pts dts设置]]></title>
    <url>%2F2019%2F04%2F04%2Fffmpeg-%E6%8E%A5%E6%94%B6rtsp%E6%97%B6%E7%9A%84pts-dts%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[设置ptsPts的设置是根据timestamp和rtcp的ntp time设置的，在函数finalize_packet()内设置。跟ntp time挂钩是音视频同步的基础。基本上可以认为timestamp的变化值就是pts的变化值，在没有B frame 时25fps 就是3600递增， 29.97fps 是3003 递增。存在B frame 时 timestamp 值 不是线性增加。 1234567891011121314151617181920212223242526272829303132333435363738/** * This was the second switch in rtp_parse packet. * Normalizes time, if required, sets stream_index, etc. */ static void finalize_packet(RTPDemuxContext *s, AVPacket *pkt, uint32_t timestamp)&#123; if (pkt-&gt;pts != AV_NOPTS_VALUE || pkt-&gt;dts != AV_NOPTS_VALUE) return; /* Timestamp already set by depacketizer */ if (timestamp == RTP_NOTS_VALUE) return; if (s-&gt;last_rtcp_ntp_time != AV_NOPTS_VALUE &amp;&amp; s-&gt;ic-&gt;nb_streams &gt; 1) &#123; int64_t addend; int delta_timestamp; /* compute pts from timestamp with received ntp_time */ delta_timestamp = timestamp - s-&gt;last_rtcp_timestamp; /* convert to the PTS timebase */ addend = av_rescale(s-&gt;last_rtcp_ntp_time - s-&gt;first_rtcp_ntp_time, s-&gt;st-&gt;time_base.den, (uint64_t) s-&gt;st-&gt;time_base.num &lt;&lt; 32); pkt-&gt;pts = s-&gt;range_start_offset + s-&gt;rtcp_ts_offset + addend + delta_timestamp; return; &#125; if (!s-&gt;base_timestamp) s-&gt;base_timestamp = timestamp; /* assume that the difference is INT32_MIN &lt; x &lt; INT32_MAX, * but allow the first timestamp to exceed INT32_MAX */ if (!s-&gt;timestamp) s-&gt;unwrapped_timestamp += timestamp; else s-&gt;unwrapped_timestamp += (int32_t)(timestamp - s-&gt;timestamp); s-&gt;timestamp = timestamp; pkt-&gt;pts = s-&gt;unwrapped_timestamp + s-&gt;range_start_offset - s-&gt;base_timestamp; &#125; 其中的s-&gt;st-&gt;time_base.den = 90000 / 44100 等， s-&gt;st-&gt;time_base.num = 1。 设置dtsDts的值是从pts的值里面选择的，在没有B帧时设置成和pts一样； 有B frame存在时，根据B frame num ， 缓存相应B frame num个数加1的pts， 缓存的pts按从小到大排列。然后从数组0位置取出来当dts。 12345678910111213delay = st-&gt;internal-&gt;avctx-&gt;has_b_frames;......if (pkt-&gt;pts != AV_NOPTS_VALUE &amp;&amp; delay &lt;= MAX_REORDER_DELAY) &#123; st-&gt;pts_buffer[0] = pkt-&gt;pts; for (i = 0; i&lt;delay &amp;&amp; st-&gt;pts_buffer[i] &gt; st-&gt;pts_buffer[i + 1]; i++) FFSWAP(int64_t, st-&gt;pts_buffer[i], st-&gt;pts_buffer[i + 1]); if(has_decode_delay_been_guessed(st)) pkt-&gt;dts = select_from_pts_buffer(st, st-&gt;pts_buffer, pkt-&gt;dts); &#125;]]></content>
      <tags>
        <tag>ffmpeg</tag>
        <tag>h264</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamoDB Clear Data]]></title>
    <url>%2F2019%2F03%2F14%2FDynamoDB-Clear-Data%2F</url>
    <content type="text"><![CDATA[原文 Depending on the size of your table this can be too expensive and result in downtime. Remember that deletes cost you the same as a write, so you’ll get throttled by your provisioned WCU. It would be much simpler and faster to just delete and recreate the table. 12345678# this uses jq but basically we&apos;re just removing # some of the json fields that describe an existing # ddb table and are not actually part of the table schema/defintionaws dynamodb describe-table --table-name $table_name | jq &apos;.Table | del(.TableId, .TableArn, .ItemCount, .TableSizeBytes, .CreationDateTime, .TableStatus, .ProvisionedThroughput.NumberOfDecreasesToday, .ProvisionedThroughput.LastIncreaseDateTime, .ProvisionedThroughput.LastDecreaseDateTime, .LocalSecondaryIndexes[].IndexSizeBytes, .LocalSecondaryIndexes[].IndexArn, .LocalSecondaryIndexes[].ItemCount)&apos; &gt; schema.json# delete the tableaws dynamodb delete-table --table-name $table_name# create table with same schema (including name and provisioned capacity)aws dynamodb create-table --cli-input-json file://schema.json If you really want to you can delete each item individually and you’re on the right track you just need to specify both the hash and range keys in your scan projection and delete command.1234567891011aws dynamodb scan \ --attributes-to-get $HASH_KEY $RANGE_KEY \ --table-name $TABLE_NAME --query &quot;Items[*]&quot; \ # use jq to get each item on its own line | jq --compact-output &apos;.[]&apos; \ # replace newlines with null terminated so # we can tell xargs to ignore special characters | tr &apos;\n&apos; &apos;\0&apos; \ | xargs -0 -t -I keyItem \ # use the whole item as the key to delete (dynamo keys *are* dynamo items) aws dynamodb delete-item --table-name $TABLE_NAME --key=keyItem If you want to get super fancy you can use the describe-table call to fetch the hash and range key to populate $HASH_KEY and $RANGE_KEY but i’ll leave that as an exercise for you.]]></content>
      <tags>
        <tag>aws</tag>
        <tag>dynamoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Remove MySQL completely]]></title>
    <url>%2F2019%2F03%2F09%2FRemove-MySQL-completely%2F</url>
    <content type="text"><![CDATA[Remove MySQL completely1. Open the Terminal 2. Use `mysqldump` to backup your databases 3. Check for MySQL processes with: `ps -ax | grep mysql` 4. Stop and kill any MySQL processes 5. Analyze MySQL on HomeBrew: `sss` 12brew remove mysqlbrew cleanup 6. Remove files: 123456sudo rm /usr/local/mysqlsudo rm -rf /usr/local/var/mysqlsudo rm -rf /usr/local/mysql*sudo rm ~/Library/LaunchAgents/homebrew.mxcl.mysql.plistsudo rm -rf /Library/StartupItems/MySQLCOMsudo rm -rf /Library/PreferencePanes/My* 7. Unload previous MySQL Auto-Login: 1launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist 8. Remove previous MySQL Configuration: 12subl /etc/hostconfig` # Remove the line MYSQLCOM=-YES- 9. Remove previous MySQL Preferences: 1234rm -rf ~/Library/PreferencePanes/My*sudo rm -rf /Library/Receipts/mysql*sudo rm -rf /Library/Receipts/MySQL*sudo rm -rf /private/var/db/receipts/*mysql* 10. Restart your computer just to ensure any MySQL processes are killed 11. Try to run mysql, **it shouldn&#39;t work**]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Xcode断点调试ffmpeg]]></title>
    <url>%2F2019%2F03%2F06%2F%E4%BD%BF%E7%94%A8Xcode%E6%96%AD%E7%82%B9%E8%B0%83%E8%AF%95ffmpeg%2F</url>
    <content type="text"><![CDATA[参考原文整理 ffmpeg-xcodeffmpeg xcode project build stepsstep 1 下载ffmpeg源码 FFmpeggit clone https://git.ffmpeg.org/ffmpeg.git FFmpeg step 2 编译 ffmpeg cd FFmpeg ./configure --enable-debug=trace | ./configure --enable-debug | ./configure --enable-debug=3 make -j8 make -j1 一个CPU编译 make -j2 二个CPU编译 问题11编译时报错 yasm/nasm not found or too old. Use --disable-yasm for a crippled build. (yasm是汇编编译器, 因为ffmpeg中为了提高效率用到了汇编指令, 比如MMX和SSE) 解决方法1brew install yasm 问题21234567891011121314151617181920212223242526 ~/github/FFmpeg # make -j8CC libavdevice/alldevices.oCC libavdevice/avdevice.oCC libavdevice/lavfi.oOBJCC libavdevice/avfoundation.oCC libavdevice/utils.oCC libavfilter/aeval.oCC libavfilter/af_acontrast.oCC libavfilter/af_acopy.oclang: error: unknown argument: &apos;-gtrace&apos;clang: error: unknown argument: &apos;-gtrace&apos;clang: error: unknown argument: &apos;-gtrace&apos;clangclang:: errorerror: : unknown argument: &apos;-gtrace&apos;unknown argument: &apos;-gtrace&apos;clang: error: unknown argument: &apos;-gtrace&apos;clang: error: unknown argument: &apos;-gtrace&apos;make: *** [libavdevice/avfoundation.o] Error 1make: *** Waiting for unfinished jobs....make: *** [libavfilter/af_acontrast.o] Error 1make: *** [libavdevice/avdevice.o] Error 1make: *** [libavdevice/alldevices.o] Error 1make: *** [libavdevice/utils.o] Error 1make: *** [libavfilter/af_acopy.o] Error 1make: *** [libavfilter/aeval.o] Error 1clang: error: unknown argument: &apos;-gtrace&apos;make: *** [libavdevice/lavfi.o] Error 1 解决方法./configure --enable-debug 问题4Segfault on macOS 10.15 “Catalina” 12$ ffmpeg Segmentation fault: 11 12$ ./ffmpeg[1] 3439 segmentation fault ./ffmpeg 解决方法./configure —extra-cflags=”-fno-stack-check” step 3 新建一个空的 xcode 项目 Create a new Xcode project 新建一个空的 xcode 项目 项目保存路径与FFmpeg同级. step 4 添加 FFmpeg 源码目录进 ffmpeg-debug 项目中 1 选中FFmpeg目录 2 拖到项目目录下 3 不要勾选下面这个选项 接着一点要选 ffmpeg-debug, 否则看代码时, 不可以跳转. 写代码时不会有提示. step 5 添加头文件搜索路径 到这里就可以实现头文件跳转了. 要等待处理完毕才可以点击头文件或者类来查看代码. Search Paths $(SRCROOT)/../FFmpeg $(SRCROOT)/../FFmpeg/libavcodec $(SRCROOT)/../FFmpeg/llibavfilter $(SRCROOT)/../FFmpeg/libavdevice $(SRCROOT)/../FFmpeg/libutil $(SRCROOT)/../FFmpeg/libformat $(SRCROOT)/../FFmpeg/libswscal $(SRCROOT)/../FFmpeg/libpostproc $(SRCROOT)/../FFmpeg/libavresample $(SRCROOT)/../FFmpeg/libswscale step 6 添加一个 target File -&gt; New -&gt; Target -&gt; Cross-platform -&gt; Other -&gt; External Build System target 命名为 ffmpeg-make step 7 修改 ffmpeg-make 源码路径配置 修改 ffmpeg-make 源码路径 ../FFmpeg step 8 修改 ffmpeg-make 命令行参数 配置命令行参数 在Xcode工具栏中选择你的target然后点击Edit Scheme 配置executable 结果 step 9 大功告成 添加断点(ffmpeg.c-&gt;main函数), 点击 run]]></content>
      <tags>
        <tag>ffmpeg</tag>
        <tag>xcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视音频编解码技术零基础学习方法]]></title>
    <url>%2F2019%2F03%2F06%2F%E8%A7%86%E9%9F%B3%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81%E6%8A%80%E6%9C%AF%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[天妒英才，不夸张的说，如果不知道雷霄骅，可能你音视频还没入门.借用新浪网友 @张新成010 的话。“你的博客纯粹是为了分享，写的很仔细，引领了多少人入门，在音视频方面有自己的见解，在当今音视频编解码封闭技术领域，你摒弃了别人的躲躲藏藏，无私奉献，你是伟大的，很多音视频方向的朋友称你为雷神，你无愧于这个称号！一个在大学里能静下心来做研究，又能无私分享的，尤其是在中国的大学里，你是难能可贵的佼佼者，至此以后，博客上不会再有更新，音视频的世界少了一颗将才。我已无语，痛心而疾，雷霄骅，雷神，感谢有你，天堂安好！”传送门]]></content>
      <tags>
        <tag>音视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零了解H264结构]]></title>
    <url>%2F2019%2F03%2F06%2F%E4%BB%8E%E9%9B%B6%E4%BA%86%E8%A7%A3H264%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[H264/AVC是广泛采用的一种编码方式。我们这边会带大家了解。从大到小排序依次是 序列，图像，片组，片，NALU，宏块，亚宏块，块，像素。 前言我们可以了解H264处于编解码层。为什么需要编码呢？比如当前屏幕是1280720.一秒24张图片.那么我们一秒的视频数据是`1280720(位像素)*24(张) / 8(1字节8位)(结果:B) / 1024(结果:KB) / 1024 (结果:MB) = 2.64MB`一秒的数据有2.64MB数据量。1分钟就会有100多MB。这对用户来说真心是灾难。所以现在我们需要一种压缩方式减小数据的大小.在更低 比特率(bps)的情况下依然提供清晰的视频。 原理H.264原始码流(裸流)是由一个接一个NALU组成，它的功能分为两层，VCL(视频编码层)和 NAL(网络提取层).VCL(Video Coding Layer) + NAL(Network Abstraction Layer). VCL：包括核心压缩引擎和块，宏块和片的语法级别定义，设计目标是尽可能地独立于网络进行高效的编码； NAL：负责将VCL产生的比特字符串适配到各种各样的网络和多元环境中，覆盖了所有片级以上的语法级别。 在VCL进行数据传输或存储之前，这些编码的VCL数据，被映射或封装进NAL单元。（NALU）。一个NALU = 一组对应于视频编码的NALU头部信息 + 一个原始字节序列负荷(RBSP,Raw Byte Sequence Payload).如图所示，上图中的NALU的头 + RBSP 就相当于一个NALU(Nal Unit),每个单元都按独立的NALU传送。H.264的结构全部都是以NALU为主，理解了NALU，就理解了H.264的结构。一个原始的H.264 NALU 单元常由 [StartCode] [NALU Header] [NALU Payload] 三部分组成，其中 Start Code 用于标示这是一个NALU 单元的开始，必须是”00 00 00 01” 或”00 00 01” NAL Header由三部分组成，forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型）。 举例来说：00 00 00 01 06: SEI信息00 00 00 01 67: 0x67&amp;0x1f = 0x07 :SPS00 00 00 01 68: 0x68&amp;0x1f = 0x08 :PPS00 00 00 01 65: 0x65&amp;0x1f = 0x05: IDR Slice RBSPRBSP 序列举例RBSP 描述 SODB与RBSPSODB 数据比特串 -&gt; 是编码后的原始数据.RBSP 原始字节序列载荷 -&gt; 在原始编码数据的后面添加了 结尾比特。一个 bit“1”若干比特“0”，以便字节对齐。 从NALU出发了解H.264里面的专业词语 1帧 = n个片1片 = n个宏块1宏块 = 16x16yuv数据 Slice(片)如图所示，NALU的主体中包含了Slice(片).一个片 = Slice Header + Slice Data片是H.264提出的新概念，通过编码图片后切分通过高效的方式整合出来的概念。一张图片有一个或者多个片，而片由NALU装载并进行网络传输的。但是NALU不一定是切片，这是充分不必要条件，因为 NALU 还有可能装载着其他用作描述视频的信息.那么为什么要设置片呢?设置片的目的是为了限制误码的扩散和传输，应使编码片相互间是独立的。某片的预测不能以其他片中的宏块为参考图像，这样某一片中的预测误差才不会传播到其他片中。 可以看到上图中，每个图像中，若干宏块(Macroblock)被排列成片。一个视频图像可编程一个或更多个片，每片包含整数个宏块 (MB),每片至少包含一个宏块。片有一下五种类型: 片 意义 I 片 只包含I宏块 P 片 包含P和I宏块 B 片 包含B和I宏块 SP 片 包含P 和/或 I宏块,用于不同码流之间的切换 SI 片 一种特殊类型的编码宏块 宏块(Macroblock)刚才在片中提到了宏块.那么什么是宏块呢？宏块是视频信息的主要承载者。一个编码图像通常划分为多个宏块组成.包含着每一个像素的亮度和色度信息。视频解码最主要的工作则是提供高效的方式从码流中获得宏块中像素阵列。一个宏块 = 一个16*16的亮度像素 + 一个8×8Cb + 一个8×8Cr彩色像素块组成。(YCbCr 是属于 YUV 家族的一员,在YCbCr 中 Y 是指亮度分量，Cb 指蓝色色度分量，而 Cr 指红色色度分量) 宏块分类 意义 I 宏块 利用从当前片中已解码的像素作为参考进行帧内预测 P 宏块 利用前面已编码图像作为参考进行帧内预测，一个帧内编码的宏块可进一步作宏块的分割:即16×16.16×8.8×16.8×8亮度像素块。如果选了8×8的子宏块，则可再分成各种子宏块的分割，其尺寸为8×8，8×4，4×8，4×4 B 宏块 利用双向的参考图像(当前和未来的已编码图像帧)进行帧内预测 在 H.264 中，句法元素共被组织成 序列、图像、片、宏块、子宏块五个层次。句法元素的分层结构有助于更有效地节省码流。例如，再一个图像中，经常会在各个片之间有相同的数据，如果每个片都同时携带这些数据，势必会造成码流的浪费。更为有效的做法是将该图像的公共信息抽取出来，形成图像一级的句法元素，而在片级只携带该片自身独有的句法元素。 宏块分类 意义 mb_type 确定该 MB 是帧内或帧间(P 或 B)编码模式，确定该 MB 分割的尺寸 mb_pred 确定帧内预测模式(帧内宏块)确定表 0 或表 1 参考图 像，和每一宏块分割的差分编码的运动矢量(帧间宏块，除 8×8 宏块分割的帧内 MB) sub_mb_pred (只对 8×8MB 分割的帧内 MB)确定每一子宏块的子宏 块分割，每一宏块分割的表 0 和/或表 1 的参考图象;每一 宏块子分割的差分编码运动矢量。 coded_block_pattern 指出哪个 8×8 块(亮度和彩色)包 编码变换系数 mb_qp_delta 量化参数的改变值 residual 预测后对应于残差图象取样的编码变换系数 图像,场和帧图像是个集合概念，顶 场、底场、帧都可以称为图像。对于H.264 协议来说，我们平常所熟悉的那些称呼，例如： I 帧、P 帧、B帧等等，实际上都是我们把图像这个概念具体化和细小化了。我们 在 H.264里提到的“帧”通常就是指不分场的图像； 视频的一场或一帧可用来产生一个编码图像。一帧通常是一个完整的图像。当采集视频信号时，如果采用隔行扫描(奇.偶数行),则扫描下来的一帧图像就被分为了两个部分,这每一部分就被称为[场],根据次序氛围: [顶场] 和 [底场]。 方式 作用域 帧编码方式 活动量较小或者静止的图像宜采用 场编码方式 活动量较大的运动图像 I,P,B帧与pts/dts 帧的分类 中文 意义 I帧 帧内编码帧,又称intra picture I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩，做为随机访问的参考点，可以当成图象。I帧可以看成是一个图像经过压缩后的产物 P帧 前向预测编码帧,又称predictive-frame 通过充分将低于图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧 B帧 双向预测帧,又称bi-directional interpolated prediction frame 既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像,也叫双向预测帧 I P B帧的不同: I frame:自身可以通过视频解压算法解压成一张单独的完整的图片。 P frame：需要参考其前面的一个I frame 或者B frame来生成一张完整的图片。 B frame:则要参考其前一个I或者P帧及其后面的一个P帧来生成一张完整的图片。 名称 | 意义PTS(Presentation Time Stamp) | PTS主要用于度量解码后的视频帧什么时候被显示出来。DTS(Decode Time Stamp) | DTS主要是标识内存中的bit流再什么时候开始送入解码器中进行解码。 DTS与PTS的不同:DTS主要用户视频的解码，在解码阶段使用。PTS主要用于视频的同步和输出，在display的时候使用。再没有B frame的时候输出顺序一样。 GOPGOP是画面组，一个GOP是一组连续的画面。GOP一般有两个数字，如M=3，N=12.M制定I帧与P帧之间的距离，N指定两个I帧之间的距离。那么现在的GOP结构是I BBP BBP BBP BB I增大图片组能有效的减少编码后的视频体积，但是也会降低视频质量，至于怎么取舍，得看需求了 IDR一个序列的第一个图像叫做 IDR 图像（立即刷新图像），IDR 图像都是 I 帧图像。I和IDR帧都使用帧内预测。I帧不用参考任何帧，但是之后的P帧和B帧是有可能参考这个I帧之前的帧的。IDR就不允许这样。比如这种情况:IDR1 P4 B2 B3 P7 B5 B6 I10 B8 B9 P13 B11 B12 P16 B14 B15 这里的B8可以跨过I10去参考P7 核心作用：H.264 引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码。 帧内预测和帧间预测帧内预测（也叫帧内压缩） 我们可以通过第 1、2、3、4、5 块的编码来推测和计算第 6 块的编码，因此就不需要对第 6 块进行编码了，从而压缩了第 6 块，节省了空间 帧间预测（也叫帧间压缩） 可以看到前后两帧的差异其实是很小的，这时候用帧间压缩就很有意义。这里涉及到几个重要的概念：块匹配，残差，运动搜索(运动估计),运动补偿. 帧间压缩最常用的方式就是块匹配(Block Matching)。找找看前面已经编码的几帧里面，和我当前这个块最类似的一个块，这样我不用编码当前块的内容了，只需要编码当前块和我找到的快的差异(残差)即可。找最想的块的过程叫运动搜索(Motion Search),又叫运动估计。用残差和原来的块就能推算出当前块的过程叫运动补偿(Motion Compensation). 参考链接新一代视频压缩编码标准H.264 关于视频的一些概念I,P，B帧和PTS，DTS的关系]]></content>
      <tags>
        <tag>h264</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 入门参考]]></title>
    <url>%2F2019%2F03%2F06%2FMarkdown-%E5%85%A5%E9%97%A8%E5%8F%82%E8%80%83%2F</url>
    <content type="text"><![CDATA[Markdown是一种轻量级标记语言，创始人为约翰·格鲁伯（英语：John Gruber）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML（或者HTML）文档”。[4]这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。 由于Markdown的轻量化、易读易写特性，并且对于图片，图表、数学式都有支持，当前许多网站都广泛使用 Markdown 来撰写帮助文档或是用于论坛上发表消息。例如：GitHub、reddit、Diaspora、Stack Exchange、OpenStreetMap 、SourceForge等。甚至Markdown能被使用来撰写电子书。传送门]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homebrew Bottles源]]></title>
    <url>%2F2019%2F03%2F02%2FHomebrew-Bottles%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Homebrew Bottles源Homebrew Bottles是Homebrew提供的二进制代码包，目前镜像站收录了以下仓库： homebrew/homebrew-core homebrew/homebrew-dupes homebrew/homebrew-games homebrew/homebrew-gui homebrew/homebrew-python homebrew/homebrew-php homebrew/homebrew-science homebrew/homebrew-versions homebrew/homebrew-x11 使用方法对于bash用户：12echo &apos;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles&apos; &gt;&gt; ~/.bash_profilesource ~/.bash_profile 对于zsh用户12echo &apos;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles&apos; &gt;&gt; ~/.zshrcsource ~/.zshrc 更多说明]]></content>
      <tags>
        <tag>homebrew</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换及重置Homebrew默认源]]></title>
    <url>%2F2019%2F03%2F02%2F%E6%9B%BF%E6%8D%A2%E5%8F%8A%E9%87%8D%E7%BD%AEHomebrew%E9%BB%98%E8%AE%A4%E6%BA%90%2F</url>
    <content type="text"><![CDATA[====== 替换及重置Homebrew默认源 ======替换brew.git:cd “$(brew —repo)”git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git&lt;/code&gt; 替换Homebrew Bottles源:参考:[[https://lug.ustc.edu.cn/wiki/mirrors/help/homebrew-bottles|替换Homebrew Bottles源]] 在中科大源失效或宕机时可以： 使用[[https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/|清华源设置参考]]。 切换回官方源：重置brew.git:cd “$(brew —repo)”git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://github.com/Homebrew/homebrew-core.git&lt;/code&gt; 注释掉bash配置文件里的有关Homebrew Bottles即可恢复官方源。重启bash或让bash重读配置文件。]]></content>
      <tags>
        <tag>homebrew</tag>
      </tags>
  </entry>
</search>
